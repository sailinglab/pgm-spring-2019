---
layout: distill
title: "Lecture 22: Bayesian non-parametrics"
description: An introduction to Bayesian non-parametrics and the Dirichlet process.
date: 2019-04-08

lecturers:
  - name: Eric Xing
    url: "https://www.cs.cmu.edu/~epxing/"

authors:
  - name: Yao Chong Lim  # author's full name
    url: "#"  # optional URL to the author's homepage
  - name: Yue Wu
    url: "#"
  - name: Dongshunyi Li
    url: "#"
  - name: Steve Landers
    url: "#"

editors:
  - name: Maruan Al-Shedivat  # editor's full name
    url: "#"  # optional URL to the editor's homepage

---

## Motivation via Clustering

## The Dirichlet distribution

## Parametric vs nonparametric
In this class, we started with parametric models, which assume all data can be represented with a fixed, finite number of parameters. Models of this type include Gaussian Mixture Modles(GMM), generalized linear models(GLMs) with exponential families and so on. 

On the other hand, number of parameters of non-parametric models can grow with sample size and may be random. An example of pure non-parameteric method is kernel density estimation, where the estimation of the probability density function(pdf) of a random variable is made solely based on a finite sample of data points and some parameters including bandwidth determining the smoothness and a kernel function for weighting the distance from the observations to a particular value the random variable may take. However, non-parameteric models are sometimes cumbersome to estimate and are inflexible, especially for data with strong patterns such as those with clusters. 

Bayesian non-parametrics is an approach combining the advantages both from parametric and non-parametric modelinbg. This type of model also allows inifite number of parameters in that the prior is a device defining a random distribution on an infinite space of possible parameters. But the finite dataset that the model will fit will only use a finite subset of those parameters. Given the Bayesian setting, we can integrate out the intermediate layers of random variables if using conjugate-priors, leaving just the hyper-parameters and the data. In this way, we bypass the need to specify the infinite space and can still do inference. By using this type of methods, we can marginalize on something hard to operationalize, focus on only on the more intuitive and direct example hyperparameters and still achieve the same statistical effect of infinite modeling. 


## The Dirichlet process

### Background 
By using Bayesian non-paramterics for mixture models, we can write down the posterior probability of the observation as following: $p(x_n|\pi,\{\mu_k\},\{\Sigma_k\})=\sum_{k=1}^{\infty}\pi_k\mathcal{N}(x_n|\mu_k,\Sigma_k)$. Instead of using a finite number of possible clusters, here we use $\infty$. But we should bare in mind that actually a finite subset of them will be used. Dirichlet prior allows us implicitly define an infinte number of parameters and not necessarily store or maintain them.  

### Construct an appropriate prior  
The process of construction starts with a 2-dimensional Dirichlet distribution paramterized by $\alpha$. We then split each dimenstion to get $K$ dimensions. For example, a 4-dimensional Dirichlet can be obtained by sampling a pair of beta parameters and apply them to the 2 components according to the splitting rule. By repeating the process, we can finally have a K-dimensional Dirichlet distribution parameterized by $\alpha$, the concentration parameter, and $K$. As $K$ goes to infinity, we get a vector with infinitely many components. This operation allows us to extrapolate a finite Dirchlet into infinite without increasing the number of parameters. 

### The Dirichlet process
Considering a mixture model, given a Dirichlet prior $\pi$ with potentially infinte number of dimensions, we accompany the infinite number of weights with infinite number of centroids drew from a distribution $H$ on some space $\Omega$, e.g. a Gaussian distribution on the real line. Each sample from $H$ will be discrete and corresponding to 1 dimension in the prior $\pi$. Then $G:=\sum_{k=1}^{\infty}\pi_k\delta_{\theta_k}$ is a discrete infinite distribution over $\Omega$, with $\delta_{\theta_k}$ as an indicator for a particular element equivalent to $\theta_k$. Intuitively, when $H$ is Gaussian, the distribution looks like a set of spikes whose positions determined by Gaussian $H$ and the hights determined by the weights $\pi$. 

The discrete distribution enables sampling centroids with some clustering effect. This cannot be achieved with sampling from a continuous distribution such as Guassian, since every sample will differ from each other. While with sampling from the Dirichlet process, chances are same posititions will be repeatively sampled. 

### Sample from the Dirichlet process 
We call the point masses in the resulting distribution 'atoms' and the corresponding weights 'sizes'. 'Sizes' are determined by the concentration parameter $\alpha$. Sampling from bigger $\alpha$ leads to atoms spread out more.  


### Properties of the Dirichlet process
Sampling from Dirichlet process results in partiions of $\Omega$, which is the space of $H$.  The partition is discrete and  difffernt mass is assigned for different partite. Denote the base measure for partite $A_1$ as $H(A_1)$. Then the mass assigned for each partitie follows the Dirichlet distribution $\textbf{Dir}(\alpha H(A_1),..,\alpha H(A_k))$, where $\alpha$ is the concentration parameter, i.e.$(P(A_1),...,P(A_k))\sim\textbf{Dir}(\alpha H(A_1),..,\alpha H(A_k))$. $k$ can $\infty$. 

Through this, we don't need to express the infinity of the dimentionality in the partition. Instead, we can express by a finite number of partitions, say $k-1$ and put all the rest into the last single partition.

### Conjugacy of the Dirichlet process 
Using the conjugacy property of $\textbf{Dir}(\alpha H(A_1),..,\alpha H(A_k))$, we can express the posterior distribution of the partitions after seeing an example easily. For example, if we see an observation in the $J^{th}$, then the posterior distribution of $(P(A_1),...,P(A_j),...,P(A_k))$ follows $\textbf{Dir}(\alpha H(A_1),...,\alpha H(A_j) + 1,...,\alpha H(A_k))$. Intuitively, we can understand it as a prenormalized mass $\alpha H(A_j)$ plus an evidence. In general, this is written as $G|X_1 = x \sim DP(\alpha + 1, \frac{\alpha H + \delta_x}{\alpha +1})$. 

### Predictive distribution
Using Dirichlet Process(DP), we can define a easy and handy predictive distribution. Instead of maintaining $(P(A_1),...,P(A_j),...,P(A_k))$ which is high dimensional, we can use Bayesian trick to integrate out variables and make sampling easier. Suppose we start from nothing. Operational, we want to do point-sampling of an atom in the space $\Omega$. 

Different from k-means where we predefined the number of k and then put all data in. Here we process data point one by one. For the first data point, we start by a new cluster and sample a parameter $theta_1$ for that cluster. Then we have our first partition: $theta_1$ and everything else $theta^c$. Let $\pi_1$ be the size of atom at $\theta_1$. Then the combined mass of all the other atoms is $\pi_* =1-\pi_1$. For the distribution over these masses, the prior is $(\pi_1, pi_* ) \sim \textbf{Dir}(0,\alpha)$.Given $H$ is continuous, the mass of a particular point is 0. After assigning the first data point to the first cluster, the posterior becomes $(\pi_1,pi_* )|X_1=\theta_1 \sim \textbf{Dir}(1,\alpha)$. 

The second data point can have 2 choices, either join the existing cluster or a new cluster. We have
\begin{align*} P(X_2=\theta_k|X_1=\theta_1)&=\intP(X_2=\theta_k,(\pi,\pi_* )X=\theta_1)d\pi_1 \\ 
&= \int P(X_2=\theta_k (\pi,1-\pi_1),X_1=\theta_1)P((\pi,1-\pi_1)|X_1=\theta_1) d\pi_1 \\
&= \int P(X_2=\theta_k|(\pi,1-\pi_1))P((\pi,1-\pi_1)|X_1=\theta_1) d\pi_1 \\ 
&= \int \pi_k\textbf{Dir}((\pi,1-\pi)|1,\alpha) d\pi_1 \\ 
&=\textbf{E}\_{textbf{Dir}(1,\alpha)}(\pi_k)\end{align*}

Thus, $P(X_2=\theta_k|X_1=\theta_1)=\frac{1}{1+\alpha}$ if $k=1$ or $\frac{\alpha}{1+\alpha}$ for new $k$. Note that here we use the posterior after seeing the first data point as prior for the second data point. And the posterior after seeing the second will be used as the prior for the third data point. Say for the second data point, we choose to start a new cluster and sample a new parameter $\theta_2$ for that cluster. Let $\pi_2$ be the size of the atom $\theta_2$. For the third data point, we use the posterior from seeing the second point $(\pi_1,\pi_2,\pi_* )|X_1 = \theta_1, X_2=\theta_2 \sim \textbf{Dir}(1,1,\alpha)$ as the prior. Using the same integration trick, we integrate out $\pi = (\pi,\pi_2,\pi_* )$ and have $P(X_3 = \theta_k|X_1=\theta_1, X_2=\theta_2)=\textbf{E}\_{\textbf{Dir}(1,1,\alpha)}(\pi_k)$. That is to say, $P(X_3 = \theta_k|X_1=\theta_1, X_2=\theta_2)$ equals to $\frac{1}{2+\alpha}$ if $k=1$ or $k=2$ and equals to $\frac{\alpha}{2+\alpha}$ for new $k$. 

By applying the integration trick over $\pi$,  we can focus on the prior distribution from the last step, which is parameterized by the number of clusters seeing so far and the counts for data points belonging to each cluster and $\alpha$. In general, if $m_k$ is the number of times we have seen $X_i=k$ and $K$ is the total number of observed values. We have:

\begin{align*} P(X_{n+1}=\theta_k|X_1,..,X_n)&=\textbf{E}\_{\textbf{Dir}(m_1,..,m_K,\alpha)}(\pi_k) \\ \end{align*}

Thus, $P(X_{n+1}=\theta_k|X_1,..,X_n)$ equals $\frac{m_k}{n+\alpha}$, the weight defines by the $k_{th}$ atom, if $k \leq K$ and it equals to $\frac{\alpha}{n+\alpha}$, an residual probability, for yet another new cluster. In this way, we can keep attributing labels of new data points purely depending on observed data points and the concentration parameter $\alpha$. $\pi$ are not needed and we don't need to worry about sampling $\pi$. However, we still need to try a few numbers to pick $\alpha$.


## Metaphors for the Dirichlet process

There are many ways to visualize the Dirichlet process. Three common metaphors are the Polya urn scheme, the Chinese restaurant process, and the stick-breaking process.

### Pólya urn scheme

<figure id="polya-gm" class="l-body-outset">
  <div class="row">
    <div class="col one">
      <img src="{{ 'assets/img/notes/lecture-22/polya_gm.png' | relative_url }}" />
      <figcaption>
      Graphical model for the Pólya urn scheme.
      </figcaption>
    </div>
  </div>
</figure>

<!-- The predictive distribution given by the Dirichlet process can be understood intuitively using the Polya urn scheme (also called the Blackwell-MacQueen urn scheme). -->

In the Pólya urn scheme (also called the Blackwell-MacQueen urn scheme)<d-cite key="blackwell1973"></d-cite>, imagine we have an urn initially containing a black ball of mass $\alpha$. To generate each sample $X_n$, sample a ball from the urn with probability proportional to its mass. The color of the ball is the partition that $X_n$ comes from. If the ball is black, choose a new previously-unseen color for the sample, and return the black ball and a unit-mass ball of that new color to the urn. If the ball is not black, just return it and another unit-mass ball of the same color to the urn.

Then, after sampling $n$ balls, the joint distribution of the balls' colors (i.e. the cluster the balls belong to) follows a Dirichlet process.

From this scheme, we can also easily see the self-reinforcing property that the Dirichlet process exhibits: new samples are more likely to be drawn from partitions that already have many existing samples.

### The Chinese restaurant process

<!-- [image?] -->

The Chinese restaurant process is an equivalent description of the Pólya urn scheme.

Imagine a restaurant with an infinite number of tables, all initially empty. The first customer enters the restaurant, and picks a table to sit at. When the $(n-1)$-th customer enters, they either 1) sit at an occupied table $k$ with probability $\frac{m_k}{n + \alpha}$, where $m_k$ is the number of people that are currently sitting at table $k$, or 2) start a new table with probability $\frac{\alpha}{n + \alpha}$. We can see that this description is equivalent to the Pólya urn scheme above.

<!-- We can generalize the Chinese restaurant process as follows: The probability now depends on both the number of people  -->

### Note on Exchangeability

Even though the two descriptions above have a notion of an _order_ of the drawn samples, it turns out that the distribution over the partitions of the first $N$ samples does not _depend on the order of the samples_. However this does not mean the samples are independent, since we have seen the self-reinforcing property of the Dirichlet process - samples tend to be drawn from partitions that already have many existing samples.

Distributions that do not depend on the order of the samples are called __exchangeable__ distributions.

De Finetti's theorem states that if a sequence of observations are exchangeable, then there must exist a distribution given which the samples are i.i.d. In this case, the samples are i.i.d. given the Dirichlet process.

### The stick-breaking process

<figure id="stick-breaking-gm" class="l-body-outset">
  <div class="row">
    <div class="col one">
      <img src="{{ 'assets/img/notes/lecture-22/stick_breaking_gm.png' | relative_url }}" />
      <figcaption>
      Graphical model for the stick-breaking process.
      </figcaption>
    </div>
  </div>
</figure>

The two metaphors above primarily show the distribution of the data points among the resulting partitions. The stick-breaking process shows how the partitions of the DP can be constructed, along with their associated parameters, to give the parameters of the resulting distribution $G$<d-cite key="sethuraman1994"></d-cite>.

<!-- Another alternative view of the Dirichlet process is the stick-breaking process to construct the partitions of the DP and their associated parameters<d-cite key="sethuraman1994"></d-cite>. -->

Imagine we begin with a stick of unit length, which represents our total probability. We then repeatedly break off fractions of the stick, and assign parameters to each broken-off fraction according to our base measure.

Concretely, to construct the $k$-th partition:
1. Sample a Beta$(1, \alpha)$ random variable $\beta_k \in [0, 1]$.
1. Break off a fraction $\beta_k$ of the remaining stick. This gives us the $k$-th partition. We can calculate its atom size $\pi_k$ using the previous fractions $\beta_1, ..., \beta_k$.
1. Sample a random parameter $\theta_k$ for the partition for this atom from our base measure $G_0$.
1. Recur on the remaining stick.

In summary,
<d-math block>
\begin{aligned}
\beta_k &\sim \text{Beta}(1, \alpha) \\
\pi_k &= \beta_k \prod_{\ell=1}^{k-1} (1 - \beta_{\ell}) \\
\theta_k &\sim G_0 \\
G &= \sum_{k=1}^{\infty} \pi_k \delta_{\theta_k} \sim \text{DP}(\alpha, G_0)
\end{aligned}
</d-math>

The stick-breaking metaphor shows us how we can approximate a Dirichlet process. The final distribution $G$ is obtained from a weighted sum of the sampled parameters $\theta_k$, weighted by the atom size $\pi_k$. Since the atom sizes of the created partitions in the stick-breaking process are monotonically decreasing, we can get a good approximation of $G$ by just using the first $k$ sampled parameters and atom sizes.

