layout: distill
title: "Integrative Paradigms of GM: Regularized Bayesian Methods"
description: An example of a distill-style lecture notes that showcases the main elements.
date: 2019-04-15

lecturers:
  - name: Eric Xing
    url: "https://www.cs.cmu.edu/~epxing/"

authors:
  - name: Zhipeng Yang
    url: "#"  # optional URL to the author's homepage
  - name: Author 2
    url: "#"
  - name: Xinze Wang
    url: "#"
  - name: Zhiqi Wang

editors:
  - name: Editor 1  # editor's full name
    url: "#"  # optional URL to the editor's homepage

abstract: >
  An example abstract block.

## Zhipeng(page1-14)

## Learning GMs
There are different frameworks to learn GMs: First, Bayesian framework. It allows priors to be introduced. Both parametric and nonparametric Bayesian tricks can be applied to learning these models. Second, max-margin framework. SVM is an example of this framework(it can be used to learn not only a classifier but also a graphical model). Third, there are also the kernel methods, of which SVM is also an example. Gaussian processes, another nonparametric bayesian paradigm, is another example of application of kernel methods. 
	
These frameworks has complementary advantages of one another. For example, in the Bayesian framework, we have prior knowledges, we can bypass model selection; in SVM, outliers do not affect the results, etc. It is possible to use the ideas of these different frameworks and enjoy the advantages of all in one single paradigm. Potentially these ideas could be used to further empower the already powerful deep-learning models, which would be an interesting new topic to explore in the future.

## Bayesian inference
We are already familiar with the Bayes' rule:
	$$p(\mathcal{M}\vert x) =  \frac{p(x\vert\mathcal{M})\pi(\mathcal{M})}{\int p(x\vert\mathcal{M})\pi(\mathcal{M})d\mathcal{M}}$$
where $\mathcal{M}$ is a model from some hypothesis space, $x$ is observed data. The Bayesian framework allows you to derive a posterior distribution of the model. The prior distribution, i.e. the $\pi(\mathcal{M})$ part of the model needs to be provided, usually selected by the needs, while the $p(\mathbf{x}\vert\mathcal{M})$ part of the model need to be designed and could be the graphical part.
	
In parametric Bayesian inference, $\mathcal{M}$ is represented as a finite set of parameters $\theta$.
    \begin{itemize}
	    \item a parametric likelihood: $\mathbf{x} \sim p(\cdot\vert\theta)$
	    \item Prior on $\theta$: $\pi(\theta)$
	    \item Posterior: $p(\theta \vert \mathbf{x}) = \frac{p(\mathbf{x}\vert\theta)\pi(\theta)}{\int    p(\mathbf{x}\vert\theta)\pi(\theta)d{\theta}} \propto p(\mathbf{x}\vert\theta)\pi(\theta)$
	\end{itemize}
You don't make too much flexibility	in the choice of the model itself. You can make a choice of a Gaussian model or a Dirichlet distribution, the flexibility is in the way how it is parameterized. Define a prior distribution of the parameters, and you'll get a posterior distribution of the parameters.
    
In nonparametric Bayesian inference, $\mathcal{M}$ is a richer model.
	\begin{itemize}
	    \item Nonparametric likelihood: $\mathbf{x} \sim p(\cdot\vert\mathcal{M})$
	    \item Prior on $\mathcal{M}$: $\pi(\mathcal{M})$
	    \item Posterior: $ p(\mathcal{M}\vert x) =  \frac{p(x\vert\mathcal{M})\pi(\mathcal{M})}{\int p(x\vert\mathcal{M})\pi(\mathcal{M})d\mathcal{M}} \propto p(x\vert\mathcal{M})\pi(\mathcal{M})$
	\end{itemize}
	 The model itself becomes a space for you to make inference on. For example you have an unknown number of components in a mixed model, or an unknown number of dimensions in the latent feature models. Popular nonparametric Bayesian models include Dirichlet Process, Indian Buffet process and Gaussian process. These models are more powerful than the parametric Bayesian models. Nonparametric Bayesian models allow us to pay more attention to the power of data, and the interplay between the data and prior is more natural. It allows us to bypass the model selection problem and let the data itself determine model complexity.
	
There is a new, different expression of Bayes' rule (Zellner, Am. Stat. 1988):
	$$\min_{p(\mathcal{M})} \text{KL}(p(\mathcal{M})\Vert \pi(\mathcal{M})) - \mathbb{E}_{p(\mathcal{M})}[log(p(x\vert\mathcal{M}))]$$
    $$\text{s.t.}: p(\mathcal{M})\in\mathcal{P}_{\text{prob}}$$
$\mathcal{P}_{\text{prob}}$ is a direct but trivial constraint on the posterior distribution. 
This variation of expression of Bayes' rule turns the problem into a optimization problem. It also gives space to inference algorithms, or even to augment the model. This new expression can be used to steer the Bayes inference into some interesting directions.
    
With this expression we can play some tricks in redefining the space of the posterior distribution. The trivial constraint $\mathcal{P}_{\text{prob}}$ that guarantees whatever distributions are allowed can also be tightened to ensure that only a subset of distributions are allowed. The subset can be defined due to constraint from data. For example:
	$$
	    \inf_{q(\mathcal{M}),\xi}\text{KL}(q(\mathcal{M})\Vert\pi(\mathcal{M}))
	    -\int_{\mathcal{M}}\log p(\mathcal{D}\vert\mathcal{M})q(\mathcal{M})d\mathcal{M}
	    + U(\xi)$$
	    $$\text{s.t.:}q(\mathcal{M})\in\mathcal{P}_\text{post}(\xi)
	$$
where, e.x.,
	$$\mathcal{P}_\text{post}(\xi) \overset{\mathrm{def}}{=} \Big\{q(M)\vert\forall t=1,\dots, T, h(Eq(\psi_t; \mathcal{D}))\leq\xi_t\Big\},$$
and $$U(\xi) = \sum_{t=1}^{T}\mathbb{I}(\xi_t=\gamma_t)=\mathbb{I}(\xi=\gamma).$$
For every data points you can constrain the distribution to satisfy the margin, and that gives you a new set of posterior distributions.

## MLE versus Max-margin learning
Now let's put MLE and Max-margin learning side by side for comparison. For likelihood-based classification, a typical example is logistic regression. And for Max-margin learning, the example would be SVM.

In the classical predictive models the input and output spaces are $$\mathcal{X}\triangleq\mathbb{R}^{M_x}\qquad \mathcal{Y}\triangleq\{-1,+1\}.$$
We are learning
$$\^{\mathbf{w}} = \text{arg}\min_{\mathbf{w}\in\mathcal{W}}\ell(x,y;\mathbf{w}+\lambda  R(\mathbf{w}))$$
where $\ell(\cdot)$ represents a convex loss, and $R(\mathbf{w})$ is a regularizer to prevent overfitting.

In logistic regression, the maximum likelihood estimation is $$\max_{\mathbf{w}}\mathcal{L}(\mathcal{D}; \mathbf{w}) \triangleq \sum_{i=1}^{N}\log p(y^i \vert x^i; \mathbf{w}) + \mathcal{N}(\mathbf{w})$$
i.e. you are maximizing the likelihood of the label given the data points. A regularizer $\mathcal{N}(\mathbf{w})$ could be introduced. This is sometimes called a shrinkage function.

It corresponds to a log loss with L2R: 
$$\ell_{LL}(x,y;\mathbf{w}) \triangleq \ln \sum_{y'\in\mathcal{Y}} \exp \{ \mathbf{w}^\top\mathbf{f}(x, y')\} -\mathbf{w}^\top\mathbf{f}(x, y)$$

SVM is formulated very differently: 
$$\min_{\mathbf{w},\xi}\frac{1}{2}\mathbf{w}^\top\mathbf{w} + C\sum_{i=1}^{N}\xi_i; \quad $$ $$\text{s.t.} \forall{i},\forall{y'\neq y}:\mathbf{w}^\top\Delta\mathbf{f}_i(y')\geq1-\xi_i,\, \xi_i\geq 0.$$
It corresponds to a hinge loss with L2R:
$$\ell_{MM}(x,y;\mathbf{w}) \triangleq \max_{y' \in \mathcal{Y}}\mathbf{w}^\top\mathbf{f}(x,y') - \mathbf{w}^\top\mathbf{f}(x,y) + \ell'(y', y)$$
	
The two learning paradigms have complementary advantages. Likelihood-based models are probablistic, therefore by introducing a prior distribution, Bayesian learning can be easily performed. Besides, probablistic models allow for introduction of latent variables so that you can have enable latent space models. SVM, on the other hand, does not allow for hidden variables because it is non-probablistic. But the advantages are that the support vector property gives good guarantee on generizability, and that it allows for kernel tricks. 
	
The Maximum Entropy Discrimination (MED) is an approach to combine the logistic regression and SVM. 
Model averaging$$\^{y} = \text{sign} \int p(\mathbf{w})F(x;\mathbf{w}), (y\in\{+1, -1\})$$
The optimization problem (binary classification) is:
$$\min_{p(\Theta)} \text{KL}(p(\Theta)\Vert p_0(\Theta))$$
$$\text{s.t.}\quad\int p(\Theta)\big[y_iF(x;\mathbf{w}-\xi_i)\big]d\Theta\geq0, \forall{i},$$
where $\Theta$ is the parameter $\mathbf{w}$ when $\xi$ are kept fixed or the pair $\mathbf{w},\xi$ when we want to optimize over $\xi$.
This is a mechanical combination of the two approaches because the margin idea is used to define constraints to the posterior, and the likelihood idea to define the loss.

## Structured Prediction Graphical Models
Input and output space:$$\mathcal{X}\triangleq\mathbb{R}_{X_1}\times,\dots,{R}_{X_K} \qquad \mathcal{Y}\triangleq \mathcal{R}_{Y_1}\times,\dots,{R}_{Y_K},$$

Conditional Random Fields (CRF) is based on a logistic loss with a max-likelihood estimation (point-estimate) of:
$$\mathcal{L}(\mathcal{D};\mathbf{w}) \triangleq \log \sum_{y'}\exp{(\mathbf{w}^\top\mathbf{f}(x,y'))}-\mathbf{w}^\top\mathbf{f}(x,y) + R(\mathbf{w})$$

Max-Margin Markov Networks are based on a hinge loss, with max-margin learning (point-estimate):
$$\mathcal{L}(\mathcal{D};\mathbf{w})\triangleq \log\max_{y'}\mathbf{w}^\top\mathbf{f}(x,y') - \mathbf{w}^\top\mathbf{f}(x,y) + \ell(y',y) + R(\mathbf{w})$$

Combine the conditional random fields and max-margin Markov networks, and we get MED-MN.

## Ninghao(page15-25)

## xinze(page26-34)

## Predictive Latent Subspace Learning via a large-margin approach

There are some latent space models, such as Latent Dirichlet Allocation(LDA)，  Principal Component Analysis(PCA). One of major utility of latent space models is to get embeddings. Most of time, we want to use the embeddings to make better predictions. Finding latent subspace representations is an old topic, which means mapping a high-dimensional representation into a latent low-dimensional representation, where each dimension can have some interpretable meaning.

In the past, the whole process can be divided into two steps. Firstly, get the embedded features of every text. Secondly, use the embedded feature as augmented data to retrain a classifier. The classification error, which can be represented as a loss function, is different from the embedding loss function. But now, we try to coalesce the two steps, which can be called as predictive subspace learning with supervision.

Unsupervised latent subspace representations are generic but can be sub-optimal for predictions. Many datasets are available with supervised side information. They can be noisy, but not random noise. For example, labels and rating scores are usually assigned based on some intrinsic property of the data. So it is helpful to suppress noise and capture the most useful aspects of the data. The final goal is to discover latent subspace representations that are both predictive and interpretable by exploring weak supervision information.

### LDA: Latent Dirichlet Allocation

As shown below, the model tries to get latent variables for every word. During the process, it needs to initialize the Dirichlet parameter, and get the latent topic vector for every documents. The generative procudure is

<figure>
  <div align="center">
  <div class="col two">
      <img src="{{ '/assets/img/notes/lecture-24/original_lda.png' | relative_url }}"/>
  </div>
</div>
  <figcaption>
    LDA Model
  </figcaption>
</figure>


> Generative Procedure:
>   * For each document $d$:
>     * Sample a topic proportion $\theta_{d} \sim Dir(\alpha)$
>     * For each word:
>       * Sample a topic $Z_{d,n} \sim Mult(\theta_{d})$
>       * Sample a word $W_{d,n} \sim Mult(\beta_{z_{d,n}})$

The joint distribution is

$$
p(\theta, \mathbf{z}, \mathbf{W} | \alpha, \beta)=\prod_{d=1}^{D} p\left(\theta_{d} | \alpha\right)\left(\prod_{n=1}^{N} p\left(z_{d n} | \theta_{d}\right) p\left(w_{d n} | z_{d n}, \beta\right)\right)
$$ 

But exact inference is intractable, and variational inference is

$$
q(\mathbf{z}, \theta) \sim p(\mathbf{z}, \theta | \mathbf{W}, \alpha, \beta)\\ 

\mathcal{L}(q) \triangleq-E_{q}[\log p(\theta, \mathbf{z}, \mathbf{W} | \alpha, \beta)]-\mathcal{H}(q(\mathbf{z}, \theta)) \geq-\log p(\mathbf{W} | \alpha, \beta)
$$

In this way, we can minimize the variational bound to estimate parameters and infer the posterior distribution.

### Maximum Entropy Discrimination LDA(MedLDA)

Use the latent representations $Z_{d,n}$ to make a prediction of the label. So that the training of LDA becomes a supervised training. And the goal is to influence $\theta$ indirectly, and let embeddings to be more oriented, to be more discriminative.

<figure>
  <div align="center">
  <div class="col two">
      <img src="{{ '/assets/img/notes/lecture-24/slda.png' | relative_url }}"/>
  </div>
</div>
  <figcaption>
    Bayesian sLDA
  </figcaption>
</figure>

MED estimation can be divided into MedLDA regression model and MedLDA classification model. In MedLDA models, they replace the likelihood function by Bayesians LDA to make it a LDA based loss function. And then plus the prediction penalty on the margin, and constrain these margins. This can be pretty flexible, since it has two types of LDA. For example, predicting the score of the service based on the latent variables of the Yelp comments can use MedLDA Regression Model, and when predicting the label, it can use MedLDA Classification Model. The objective function considers predictive accuracy and model fitting. So it augments the original LDA by adding some new components.

The objective function and constraints for MedLDA Regression Model is

$$
P_{1}(MedLDA^{r}): \min_{q, \alpha, \beta, \delta^{2}, \xi, \xi^{*}} \mathcal{L}(q)+C \sum_{d=1}^{D}\left(\xi_{d}+\xi_{d}^{*}\right)\\

\text {s.t. } \forall d : \left\{\begin{array}{ll}{y_{d}-E\left[\eta^{T} \overline{Z}_{d}\right] \leq \epsilon-\xi_{d},} & {\mu_{d}} \\ {-y_{d}-E\left[\eta^{T} \overline{Z}_{d}\right] \leq \epsilon+\xi_{d}^{*},} & {\mu_{d}^{*}} \\ {\xi_{d} \geq 0,} & {v_{d}} \\ {\xi_{d}^{*} \geq 0,} & {v_{d}^{*}}\end{array}\right.
$$

The objective function and constraints for MedLDA Classification Model is

$$
\mathrm{P} 2\left(\operatorname{MedLDA}^{c}\right) : \min _{q, q(\eta), \alpha, \beta, \xi} \mathcal{L}(q)+C \sum_{d=1}^{D} \xi_{d}\\

\text { s.t. } \forall d, y \neq y_{d} : \quad E\left[\eta^{\top} \Delta \mathbf{f}_{d}(y)\right] \geq 1-\xi_{d} ; \xi_{d} \geq 0
$$

### Comparision between LDA and supervised LDA

The embedding performance of original LDA and supervised LDA can be seen from Figure in the experiment of document modeling. The embeddings of original LDA are less discrimitive, which is shown that different colors overlap with each other. But embeddings from supervised LDA are more separatly. Therefore, MedLDA makes the classification problem easier.

<figure>
  <div align="center">
  <div class="col two">
      <img src="{{ '/assets/img/notes/lecture-24/document_modeling.png' | relative_url }}"/>
  </div>
</div>
  <figcaption>
    Document Modeling
  </figcaption>
</figure>

1. Classification

In classification problems, the baseline is LDA + SVM, which has two separate steps. Models of sLDA and DiscLDA are probabilistic supervised topic models, and models of MedLDA and MedLDA + SVM are maximum margin based SVM models. And the models based on maximum margin principle perform best. The measurement is relative improvement ratio.

$$
R R(\mathcal{M})=\frac{\text { precision }(\mathcal{M})}{\text {precision}(L D A+S V M)}-1
$$

<figure>
  <div align="center">
  <div class="col two">
      <img src="{{ '/assets/img/notes/lecture-24/classification.png' | relative_url }}"/>
  </div>
</div>
  <figcaption>
    Classification Comparision
  </figcaption>
</figure>

2. Regression

The performance of regression is same as classification. The combination of likelihood and margin based procedure has the best performance. The measurement is predictive $R^{2}$ and per-word log-likelihood.

$$
p R^{2}=1-\frac{\sum_{d}(y_{d}-\hat{y}_{d})^{2}}{\sum_{d}(y_{d}-\bar{y_{d}})^{2}}
$$

<figure>
  <div align="center">
  <div class="col two">
      <img src="{{ '/assets/img/notes/lecture-24/regression.png' | relative_url }}"/>
  </div>
</div>
  <figcaption>
    Regression Comparision
  </figcaption>
</figure>

3. Time Efficiency

The time efficiency of MedLDA is pretty good, which is shown in Figure . It is much faster than the pure probabilistic version. Since MedLDA has some tricks that can be introduced in the optimization algorithm for the SVM + LDA.

<figure>
  <div align="center">
  <div class="col one">
      <img src="{{ '/assets/img/notes/lecture-24/efficiency.png' | relative_url }}"/>
  </div>
</div>
  <figcaption>
    Time Efficiency Comparision
  </figcaption>
</figure>

## zhiqi(page35-48)

## Infinite SVM

Another example invloves exploring the large margin idea in combination with non-parametric bayesian model for classfication and feature selection. In the field of classification problems, it's common to use mixture of classifiers. Conceptually, mixture of SVMs can be regarded as a combination of SVMs with different weights, and prefered over logistic regression since they have kernel functions. Here, we are talking about making priors over mixture of SVMs, and use infinite SVMs here.

Given the general theoretical framework of RegBayes,

$$
\min _{p(\mathcal{M}), \xi} \operatorname{KL}(p(\mathcal{M}) \| \pi(\mathcal{M}))-\sum_{n=1}^N \int \log p\left(\mathbf{x}_{n} | \mathcal{M}\right) p(\mathcal{M}) d \mathcal{M}+U(\xi)
$$

$$
\text { s.t. } : p(\mathcal{M}) \in \mathcal{P}_{\text { post }}(\xi)
$$

In the case of inifite SVM:

- Prior - Dirichlet process(DP)
- Model - Latent class model
- Likelihood - Gaussian likelihood
- Posterior constraints- Max-margin constraints

Inifinite SVM is the first attempt to integrate Bayesian nonparametric, large-margin learning and kernel methods. The SVMs are treated as density functions to define likelihood of the data. The detailed process is as following.

1. DP mixture of large-margin classifiers. This is the process to determine which classifier to use.

2. Given a component classifier:

   $$
   F(y, \mathbf{x} ; z, \eta)=\eta_{z}^{\top} \mathbf{f}(y, \mathbf{x})=\sum_{i=1}^{\infty} \delta_{z, i} \eta_{i}^{\top} \mathbf{f}(y, \mathbf{x})
   $$

3. Overall discriminant function:

   $$
   F(y, \mathbf{x})=\mathrm{E}_{q(z, \eta)}[F(y, \mathbf{x} ; z, \eta)]==\sum_{i=1}^{\infty} q(z=i) \mathrm{E}_{q}\left[\eta_{i}\right]^{\top} \mathrm{f}(y, \mathrm{x})
   $$

4. Prediction rule:

   $$
   y^{*}=\arg \max _{y} F(y, \mathbf{x})
   $$

5. Learning problem:

   $$
   \min _{q(\mathbf{z}, \boldsymbol{\eta})} \mathrm{KL}\left(q(\mathbf{z}, \boldsymbol{\eta}) \| p_{0}(\mathbf{z}, \boldsymbol{\eta})\right)+C_{1} \mathcal{R}(q(\mathbf{z}, \boldsymbol{\eta}))
   $$

$$
\mathcal{R}(q(\mathbf{z}, \boldsymbol{\eta}))=\sum_{d} \max _{y}\left(\ell_{d}^{\Delta}(y)+F\left(y, \mathbf{x}_{d}\right)-F\left(y_{d}, \mathbf{x}_{d}\right)\right)
$$

With assumption and relaxation, we can make the question more easier by approximating the variational distribution:

$$
q(\mathbf{z}, \boldsymbol{\eta}, \boldsymbol{\gamma}, \mathbf{v})=\prod_{d=1}^{D} q\left(z_{d}\right) \prod_{t=1}^{T} q\left(\eta_{\iota}\right) \prod_{t=1}^{T} q\left(\gamma_{\iota}\right) \prod_{t=1}^{T-1} q\left(v_{\iota}\right)
$$

And the optimization can be solved with coordinate descent. For $$q(\eta)$$, we solve an SVM learning problem, and for $$q(z)$$, we get the closed update rule:

$$
q\left(z_{d}=\iota\right) \propto \exp \left\{\left(\mathbb{E}\left[\log v_{t}\right]+\sum_{i=1}^{t-1} \mathbb{E}\left[\log \left(1-v_{i}\right)\right]\right)+\rho\left(\mathbb{E}\left[\gamma_{t}\right]^{\top} \mathbf{x}_{d}-\mathbb{E}\left[A\left(\gamma_{t}\right)\right]\right)+(1-\rho) \sum_{y} \omega_{d}^{y} \mu_{t}^{\top} \mathbf{f}_{d}^{\Delta}(y)\right\}
$$

Compared to infinite SVM, which is a bayesian nonparametric latent class model, infinite latent SVM is a Bayesian nonparametric latent feature/factor model, and each data point is mapped to a set of latent factors. The prior we use here is Indian Buffet process instead of a DP prior (used in infinite SVM).  Nonparametric IBP prior allows the models to have an unbounded number of latent features. The regularized inference problem can be efficiently solved with an iterative procedure, which leverages existing high-performance convex optimization techniques.

The experiments showed increased performance on TRECVID2003 and Flickr image datasets.

## Equations

This theme supports rendering beautiful math in inline and display modes using [KaTeX](https://khan.github.io/KaTeX/) engine.
You just need to surround your math expression with `$`, like `$ E = mc^2 $`.
If you leave it inside a paragraph, it will produce an inline expression, just like $E = mc^2$.

To use display mode, again surround your expression with `$$` and place it as a separate paragraph.
Here is an example:
$$
\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)
$$

Alternatively and for more complex math environments, use `<d-math block>...</d-math>` tags.
Here is an example:

<d-math block>
\begin{aligned}
\left( \sum_{i=1}^n u_i v_i \right)^2 & \leq \left( \sum_{i=1}^n u_i^2 \right) \left( \sum_{i=1}^n v_i^2 \right) \\
\left| \int f(x) \overline{g(x)} dx \right|^2 & \leq  \int |f(x)|^2 dx \int |g(x)|^2 dx
\end{aligned}
</d-math>

Note that [KaTeX](https://khan.github.io/KaTeX/) is work in progress, so it does not support the full range of math expressions as, say, [MathJax](https://www.mathjax.org/).
Yet, it is [blazing fast](http://www.intmath.com/cg5/katex-mathjax-comparison.php).

***

## Figures

To add figures, use `<figure>...</figure>` tags.
Within the tags, define multiple rows of images using `<div class="row">...</div>`.
To add captions, use `<figcaption>...</figcaption>` tags.

Here is an example usage of a figure that consists of a row of images with a caption:

<figure id="example-figure" class="l-body-outset">
  <div class="row">
    <div class="col one">
      <img src="{{ 'assets/img/notes/template/1.jpg' | relative_url }}" />
    </div>
    <div class="col one">
      <img src="{{ 'assets/img/notes/template/2.jpg' | relative_url }}" />
    </div>
    <div class="col one">
      <img src="{{ 'assets/img/notes/template/3.jpg' | relative_url }}" />
    </div>
  </div>
  <figcaption>
    <strong>Figure caption title in bold.</strong>
    An example figure caption.
  </figcaption>
</figure>

Note that the figure uses `class="l-body-outset"` which lets it take more horizontal space.
For more on this, see layouts section below.
Also, the size of the images themselves is controlled by `class="one"`, `class="two"`, or `class="three"` which corresponds to 1/3, 2/3, 3/3 of the full horizontal space, respectively.

Here is the same example, but each image is captioned separately:
<figure id="example-figure" class="l-body-outset">
  <div class="row">
    <div class="col one">
      <img src="{{ 'assets/img/notes/template/1.jpg' | relative_url }}" />
      <figcaption>
        <strong>Figure caption title 1.</strong>
        Caption text for figure 1.
      </figcaption>
    </div>
    <div class="col one">
      <img src="{{ 'assets/img/notes/template/2.jpg' | relative_url }}" />
      <figcaption>
        <strong>Figure caption title 2.</strong>
        A very very very long caption text for figure 2 so that it is longer than the image itself.
      </figcaption>
    </div>
  </div>
</figure>

Here is an example that shows how the figures of different sizes are aligned:

<figure>
  <div class="row">
    <div class="col two">
      <img src="{{ 'assets/img/notes/template/4.jpg' | relative_url }}" />
    </div>
    <div class="col one">
      <img src="{{ 'assets/img/notes/template/2.jpg' | relative_url }}" />
      <figcaption>
        <strong>Subcaption.</strong>
        The content of the subcaption.
      </figcaption>
    </div>
  </div>
  <figcaption>
    <strong>The second row figure caption title.</strong>
    An example of a sencond row figure caption.
  </figcaption>
</figure>
***

## Citations

Citations are then used in the article body with the `<d-cite>` tag.
The key attribute is a reference to the id provided in the bibliography.
The key attribute can take multiple ids, separated by commas.

The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover).
If you have an appendix, a bibliography is automatically created and populated in it.

Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover.
However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.

***

## Footnotes

Just wrap the text you would like to show up in a footnote in a `<d-footnote>` tag.
The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote>

***

## Code Blocks

Syntax highlighting is provided within `<d-code>` tags.
An example of inline code snippets: `<d-code language="html">let x = 10;</d-code>`.
For larger blocks of code, add a `block` attribute:

<d-code block language="javascript">
  var x = 25;
  function(x) {
    return x * x;
  }
</d-code>

***

## Layouts

The main text column is referred to as the body.
It is the assumed layout of any direct descendants of the `d-article` element.

<div class="fake-img l-body">
  <p>.l-body</p>
</div>

For images you want to display a little larger, try `.l-page`:

<div class="fake-img l-page">
  <p>.l-page</p>
</div>

All of these have an outset variant if you want to poke out from the body text a little bit.
For instance:

<div class="fake-img l-body-outset">
  <p>.l-body-outset</p>
</div>

<div class="fake-img l-page-outset">
  <p>.l-page-outset</p>
</div>

Occasionally you’ll want to use the full browser width.
For this, use `.l-screen`.
You can also inset the element a little from the edge of the browser by using the inset variant.

<div class="fake-img l-screen">
  <p>.l-screen</p>
</div>
<div class="fake-img l-screen-inset">
  <p>.l-screen-inset</p>
</div>

The final layout is for marginalia, asides, and footnotes.
It does not interrupt the normal flow of `.l-body` sized text except on mobile screen sizes.

<div class="fake-img l-gutter">
  <p>.l-gutter</p>
</div>
***

## Arbitrary $\LaTeX$ (experimental)

In fact, you can write entire blocks of LaTeX using `<latex-js>...</latex-js>` tags.
Below is an example:<d-footnote>If you don't see anything, it means that your browser does not support Shadow DOM.</d-footnote>

<latex-js style="border: 1px dashed #aaa;">
This document will show most of the supported features of \LaTeX.js.

\section{Characters}

It is possible to input any UTF-8 character either directly or by character code
using one of the following:

\begin{itemize}
    \item \texttt{\textbackslash symbol\{"00A9\}}: \symbol{"00A9}
    \item \verb|\char"A9|: \char"A9
    \item \verb|^^A9 or ^^^^00A9|: ^^A9 or ^^^^00A9
\end{itemize}

\bigskip

\noindent
Special characters, like those:
\begin{center}
\$ \& \% \# \_ \{ \} \~{} \^{} \textbackslash % \< \>  \"   % TODO cannot be typeset
\end{center}
%
have to be escaped.

More than 200 symbols are accessible through macros. For instance: 30\,\textcelsius{} is
86\,\textdegree{}F.
</latex-js>

Note that you can easily interleave latex blocks with the standard markdown.

<latex-js style="border: 1px dashed #aaa;">
\section{Environments}

\subsection{Lists: Itemize, Enumerate, and Description}

The \texttt{itemize} environment is suitable for simple lists, the \texttt{enumerate} environment for
enumerated lists, and the \texttt{description} environment for descriptions.

\begin{enumerate}
    \item You can nest the list environments to your taste:
        \begin{itemize}
            \item But it might start to look silly.
            \item[-] With a dash.
        \end{itemize}
    \item Therefore remember: \label{remember}
        \begin{description}
            \item[Stupid] things will not become smart because they are in a list.
            \item[Smart] things, though, can be presented beautifully in a list.
        \end{description}
    \item Technical note: Viewing this in Chrome, however, will show too much vertical space
        at the end of a nested environment (see above). On top of that, margin collapsing for inline-block
        boxes is not allowed. Maybe using \texttt{dl} elements is too complicated for this and a simple nested
        \texttt{div} should be used instead.
\end{enumerate}
%
Lists can be deeply nested:
%
\begin{itemize}
  \item list text, level one
    \begin{itemize}
      \item list text, level two
        \begin{itemize}
          \item list text, level three

            And a new paragraph can be started, too.
            \begin{itemize}
              \item list text, level four
    
                And a new paragraph can be started, too.
                This is the maximum level.
    
              \item list text, level four
            \end{itemize}
    
          \item list text, level three
        \end{itemize}
      \item list text, level two
    \end{itemize}
  \item list text, level one
  \item list text, level one
\end{itemize}

\section{Mathematical Formulae}

Math is typeset using KaTeX. Inline math:
$
f(x) = \int_{-\infty}^\infty \hat f(\xi)\,e^{2 \pi i \xi x} \, d\xi
$
as well as display math is supported:
$$
f(n) = \begin{cases} \frac{n}{2}, & \text{if } n\text{ is even} \\ 3n+1, & \text{if } n\text{ is odd} \end{cases}
$$

</latex-js>

Full $$\LaTeX$$ blocks are supported through [LaTeX JS](https://latex.js.org/){:target="\_blank"} library, which is still under development and supports only limited functionality (which is still pretty cool!) and does not allow fine-grained control of the layout, fonts, etc.

*Note: We do not recommend using using LaTeX JS for writing lecture notes at this stage.*

***

## Print

Finally, you can easily get a PDF or printed version of the notes by simply hitting `ctrl+P` (or `⌘+P` on macOS).

