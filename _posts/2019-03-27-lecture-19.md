---
layout: distill
title: "Lecture 19: Case Study: Text Generation"
description: Introduction to text generation as a case study for deep learning and generative modeling.
date: 2019-03-27

lecturers:
  - name: Eric Xing
    url: "https://www.cs.cmu.edu/~epxing/"

authors:
  - name: Zhengbao Jiang  # author's full name
    url: "#"  # optional URL to the author's homepage
  - name: Michael Anastos
    url: "#"
  - name: Terrance Liu
    url: "#"
  - name: Dong Wang
    url: "#"

editors:
  - name: Xun Zheng  # editor's full name
    url: "#"  # optional URL to the editor's homepage


(MLE)
<!-- Zhengbao -->


<!-- Michael -->

*Unsupervised Controlled Generation of Text*
(Controlled text generation)
<!-- Terrance -->


<!-- Dong -->

**Text Content Manipulation**
Another task is text content manipulation task,  the goal of the task is to generate a new realistic sentence yˆ that achieves (1) content fidelity by accurately describing the full content in x, and at the same time (2) style preservation by retaining as much of the writing style and characteristics of reference y′ as possible. The task is unsupervised as there is no ground-truth sentence for training.
.Below is a example,we want to rephrase the statistics of LeBron James and Irvine 
into a sentence in the style of the reference sentence "player 1 lead the way with x statitics while
 player 2 with statistics of y".

<!--
 Here is an example usage of a figure that consists of a row of images with a caption:
-->

<figure id="example-figure" class="l-body-outset">
  <div class="row">
      <img src="{{ 'assets/img/notes/lecture-19/example_content_manipulation.png' | relative_url }}" />
  </div>
  <figcaption>
    <strong>Figure caption title in bold.</strong>
    An example of text content manipulation
  </figcaption>
</figure>

We can see there is no direct supervisiong data for this task. The keg idea proposed in <d-cite key="wang2019toward"></d-cite> is to decompose the task into competitive sub-objectives
and then use direct supervision for each of the sub-objectives.  Specifically, for the unsupervised text content manipulation task, the 
basic idea is to jointly optimzie two objectives: 1)experss the original
 content corretly 2)in the same writing style as the reference sentence. 
 
 Below is an example some sample output by this method and other methods<d-cite key="logeswaran2018content"></d-cite>.
 <figure id="example-figure" class="l-body-outset">
  <div class="row">
      <img src="{{ 'assets/img/notes/lecture-19/comparison_different_models_content_manipulation.png' | relative_url }}" />
  </div>
  <figcaption>
    <strong>Figure caption title in bold.</strong>
    Sample output of different models.
  </figcaption>
</figure>

<!--
Some Markdown text with <span style="color:blue">some *blue* text</span>.
-->

Text of erroneous content is highlighted in <span style="color:red">red</span>, where <span style="color:red">[...]</span> indicates desired content is missing. Text portions in the reference sentences and the generated sentences by our model that fulfill the stylistic characteristics are highlighted in <span style="color:blue">blue</span>. Please see the text for more details.

We can see that other methods tend to keep irrelevant content originally in the reference sentence (e.g., "and 5 rebounds" in the second case), or miss necessary information in the record (e.g., one of the player names was missed in the third case). The proposed model performs better in properly adding or deleting text portions for accurate content descriptions.
Below are some quantitive evaluation results and human evaluation results with other models <d-cite key="logeswaran2018content"></d-cite> <d-cite key="sutskever2014seq2seq"></d-cite><d-cite key="subramanian2019multiple"></d-cite>.

<figure id="example-figure" class="l-body-outset">
  <div class="row">
    <div class="col one">
      <img src="{{ 'assets/img/notes/lecture-19/automatic_evaluation_model_performance.png' | relative_url }}" />
      <figcaption>
        Model Performance under Automatic Evaluation.
      </figcaption>
    </div>
    <div class="col one">
      <img src="{{ 'assets/img/notes/template/2.jpg' | relative_url }}" />
      <figcaption>
      Human Evaluation Results
      </figcaption>
    </div>
  </div>
</figure>	

In automatic evaluation results, the model<d-cite key="wang2019toward"></d-cite> proposed here  achieves significant higher content precision and recall compared to both rule-based and style transfer methods, and reaches a high BLEU score in style preservation.
In human evaluation results, the rule-based method reaches the maximum possible scores in terms of style preservation and fluency, but a much lower score in terms of content fidelity. Our model is more bal- anced across all aspects, and performs significantly better in accurately describing desired content. 

  
 
 <!--
 the goal of the task is basically to generate a sentence to describe the content in  a given data record. Generally there are many ways to experss the content, 
however we want to generate a sentence in a specific style as like a given reference
sentence.
-->
 
 

**Target-guided Open-domain Conversation**
<!--
There are two kinds of task in target-guided Open-domain Conversation.
- Task-oriented dialog
- Open-domain chit-chat

Here we define a task that is between the above two tasks. 
	Target-guided Open-domain Conversation
-->
 
 
 
Target Guided conversation can be classified to the three classes below:
- Task-oriented dialog:
    address a specific task, typically in close domain, e.g., service bot for booking a flight in
    the domain of flight service.  

- Open-domain chit-chat: improve user engagement, the conversation is random and hard to control, e.g., Apple Siri and Amazon Alex.

- Target-guided conversation: something between the previous two tasks.the conversation is in open-domain and we want to control the conversation strategy to reach a desired topic in the end of conversation. It can bridge task-oriented dialog and open-domain chit-chat, e.g. conversational reccommender system, education, psychotherapy.

and here we discuss the target-guided conversation task, in this task we want our agents can starting from any topic and reach a desired topic in the end of conversation with smooth and natural transitions. A successful example of target-guided conversation
 is shown below, we can see the agent can change the conversation topic from the starting "tired" to our final desired "e-books" naturally and smoothly.
  <figure id="example-figure" class="l-body-outset">
  <div class="row">
      <img src="{{ 'assets/img/notes/lecture-19/comparison_different_models_content_manipulation.png' | relative_url }}" />
  </div>
  <figcaption>
    <strong>Figure caption title in bold.</strong>
    Sample output of different models.
  </figcaption>
</figure>

 
 
 
 The challenge to do this open-domain target-guided conversation is that there is no direct supervised data for the data, the conversation generation is 
 totally unsupervised. To solve this challenge, we still use the previous idea, i.e, decompose the task into competitive sub-objectives and apply supervision methods on the sub-objectives. Partically, we have to achieve the subgoal of the conversation being natural and
 smooth and the subgoal that we reach the desired topic in the end. To make the conversation smooth and natural, we use  chit-chat data to learn smooth single-turn transition; to reach desired target topic, we use rule-based multi-turn planning to restrict the 
 agent to choose specific conversation topics at next step.
 
 Here is a diagram showing how target-guided open-domain conversation agent works.
   <figure id="example-figure" class="l-body-outset">
  <div class="row">
      <img src="{{ 'assets/img/notes/lecture-19/target-guided-open-domain-conversation-diagram.png' | relative_url }}" />
  </div>
  <figcaption>
    <strong>Figure caption title in bold.</strong>
    Sample output of different models.
  </figcaption>
</figure>
 
 
 As the diagram shows, to achieve the open-domain target guided conversation. Given human utterance, we first extract the keyword and then we use choose next conversation topics based on learned kernel-based topic transition and target-guided rule, and then retrieve the conditional 
 response for the next step conditioned on the keywords. When generating the keyword(s) for next step's conversation topic(s), we can tune the relative weight of the two subgoals to control how relativly important to achieve smooth topic transition and to get closer to the keyword(s) of the target topic(s).
 
 On the left of below is a successful example where the target topic is "dance" and we can see the transition is smooth and the conversation is natural.
 On the right is an example where the agent fails 
 
 Here is the same example, but each image is captioned separately:
<figure id="example-figure" class="l-body-outset">
  <div class="row">
    <div class="col one">
      <img src="{{ 'assets/img/notes/lecture-19/open_domain_conv_successful_case.png' | relative_url }}" />
      <figcaption>
        <strong>Figure caption title 1.</strong>
        Caption text for figure 1.
      </figcaption>
    </div>
    <div class="col one">
      <img src="{{ 'assets/img/notes/lecture-19/open_domain_conv_failure_case.png' | relative_url }}" />
      <figcaption>
        <strong>Figure caption title 2.</strong>
        A very very very long caption text for figure 2 so that it is longer than the image itself.
      </figcaption>
    </div>
  </div>
</figure>	
 
 In summary, for the three tasks in unsupervised controlled generation of text. We can decompose the task into competive sub-objectives and jointly train the sub-objectives with direct supervision.
  
**Summary**

We have two central goal for text generating tasks.

- Generating human-like, grammatical, and readable text I.e., generating natural language
- Generating text that contains desired information inferred from inputs Machine translation

    - Machine translation: Source sentence --> target sentence w/ the same meaning
    - Data description: Table --> data report describing the table
    - Attribute control: Sentiment: positive --> "I like this restaurant"
    - Conversation control: Control conversation strategy and topic
Source sentence --> target sentence w/ the same meaning
	
	
	

	

abstract: >
  An example abstract block.
---

## Equations

This theme supports rendering beautiful math in inline and display modes using [KaTeX](https://khan.github.io/KaTeX/) engine.
You just need to surround your math expression with `$`, like `$ E = mc^2 $`.
If you leave it inside a paragraph, it will produce an inline expression, just like $E = mc^2$.

To use display mode, again surround your expression with `$$` and place it as a separate paragraph.
Here is an example:

$$
\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)
$$

Alternatively and for more complex math environments, use `<d-math block>...</d-math>` tags.
Here is an example:

<d-math block>
\begin{aligned}
\left( \sum_{i=1}^n u_i v_i \right)^2 & \leq \left( \sum_{i=1}^n u_i^2 \right) \left( \sum_{i=1}^n v_i^2 \right) \\
\left| \int f(x) \overline{g(x)} dx \right|^2 & \leq  \int |f(x)|^2 dx \int |g(x)|^2 dx
\end{aligned}
</d-math>

Note that [KaTeX](https://khan.github.io/KaTeX/) is work in progress, so it does not support the full range of math expressions as, say, [MathJax](https://www.mathjax.org/).
Yet, it is [blazing fast](http://www.intmath.com/cg5/katex-mathjax-comparison.php).

***

## Figures

To add figures, use `<figure>...</figure>` tags.
Within the tags, define multiple rows of images using `<div class="row">...</div>`.
To add captions, use `<figcaption>...</figcaption>` tags.

Here is an example usage of a figure that consists of a row of images with a caption:

<figure id="example-figure" class="l-body-outset">
  <div class="row">
    <div class="col one">
      <img src="{{ 'assets/img/notes/template/1.jpg' | relative_url }}" />
    </div>
    <div class="col one">
      <img src="{{ 'assets/img/notes/template/2.jpg' | relative_url }}" />
    </div>
    <div class="col one">
      <img src="{{ 'assets/img/notes/template/3.jpg' | relative_url }}" />
    </div>
  </div>
  <figcaption>
    <strong>Figure caption title in bold.</strong>
    An example figure caption.
  </figcaption>
</figure>

Note that the figure uses `class="l-body-outset"` which lets it take more horizontal space.
For more on this, see layouts section below.
Also, the size of the images themselves is controlled by `class="one"`, `class="two"`, or `class="three"` which corresponds to 1/3, 2/3, 3/3 of the full horizontal space, respectively.

Here is the same example, but each image is captioned separately:
<figure id="example-figure" class="l-body-outset">
  <div class="row">
    <div class="col one">
      <img src="{{ 'assets/img/notes/template/1.jpg' | relative_url }}" />
      <figcaption>
        <strong>Figure caption title 1.</strong>
        Caption text for figure 1.
      </figcaption>
    </div>
    <div class="col one">
      <img src="{{ 'assets/img/notes/template/2.jpg' | relative_url }}" />
      <figcaption>
        <strong>Figure caption title 2.</strong>
        A very very very long caption text for figure 2 so that it is longer than the image itself.
      </figcaption>
    </div>
  </div>
</figure>

Here is an example that shows how the figures of different sizes are aligned:

<figure>
  <div class="row">
    <div class="col two">
      <img src="{{ 'assets/img/notes/template/4.jpg' | relative_url }}" />
    </div>
    <div class="col one">
      <img src="{{ 'assets/img/notes/template/2.jpg' | relative_url }}" />
      <figcaption>
        <strong>Subcaption.</strong>
        The content of the subcaption.
      </figcaption>
    </div>
  </div>
  <figcaption>
    <strong>The second row figure caption title.</strong>
    An example of a sencond row figure caption.
  </figcaption>
</figure>

***

## Citations

Citations are then used in the article body with the `<d-cite>` tag.
The key attribute is a reference to the id provided in the bibliography.
The key attribute can take multiple ids, separated by commas.

The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover).
If you have an appendix, a bibliography is automatically created and populated in it.

Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover.
However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.

***

## Footnotes

Just wrap the text you would like to show up in a footnote in a `<d-footnote>` tag.
The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote>

***

## Code Blocks

Syntax highlighting is provided within `<d-code>` tags.
An example of inline code snippets: `<d-code language="html">let x = 10;</d-code>`.
For larger blocks of code, add a `block` attribute:

<d-code block language="javascript">
  var x = 25;
  function(x) {
    return x * x;
  }
</d-code>

***

## Layouts

The main text column is referred to as the body.
It is the assumed layout of any direct descendants of the `d-article` element.

<div class="fake-img l-body">
  <p>.l-body</p>
</div>

For images you want to display a little larger, try `.l-page`:

<div class="fake-img l-page">
  <p>.l-page</p>
</div>

All of these have an outset variant if you want to poke out from the body text a little bit.
For instance:

<div class="fake-img l-body-outset">
  <p>.l-body-outset</p>
</div>

<div class="fake-img l-page-outset">
  <p>.l-page-outset</p>
</div>

Occasionally you’ll want to use the full browser width.
For this, use `.l-screen`.
You can also inset the element a little from the edge of the browser by using the inset variant.

<div class="fake-img l-screen">
  <p>.l-screen</p>
</div>
<div class="fake-img l-screen-inset">
  <p>.l-screen-inset</p>
</div>

The final layout is for marginalia, asides, and footnotes.
It does not interrupt the normal flow of `.l-body` sized text except on mobile screen sizes.

<div class="fake-img l-gutter">
  <p>.l-gutter</p>
</div>

***

## Arbitrary $$\LaTeX$$ (experimental)

In fact, you can write entire blocks of LaTeX using `<latex-js>...</latex-js>` tags.
Below is an example:<d-footnote>If you don't see anything, it means that your browser does not support Shadow DOM.</d-footnote>

<latex-js style="border: 1px dashed #aaa;">
This document will show most of the supported features of \LaTeX.js.

\section{Characters}

It is possible to input any UTF-8 character either directly or by character code
using one of the following:

\begin{itemize}
    \item \texttt{\textbackslash symbol\{"00A9\}}: \symbol{"00A9}
    \item \verb|\char"A9|: \char"A9
    \item \verb|^^A9 or ^^^^00A9|: ^^A9 or ^^^^00A9
\end{itemize}

\bigskip

\noindent
Special characters, like those:
\begin{center}
\$ \& \% \# \_ \{ \} \~{} \^{} \textbackslash % \< \>  \"   % TODO cannot be typeset
\end{center}
%
have to be escaped.

More than 200 symbols are accessible through macros. For instance: 30\,\textcelsius{} is
86\,\textdegree{}F.
</latex-js>

Note that you can easily interleave latex blocks with the standard markdown.

<latex-js style="border: 1px dashed #aaa;">
\section{Environments}

\subsection{Lists: Itemize, Enumerate, and Description}

The \texttt{itemize} environment is suitable for simple lists, the \texttt{enumerate} environment for
enumerated lists, and the \texttt{description} environment for descriptions.

\begin{enumerate}
    \item You can nest the list environments to your taste:
        \begin{itemize}
            \item But it might start to look silly.
            \item[-] With a dash.
        \end{itemize}
    \item Therefore remember: \label{remember}
        \begin{description}
            \item[Stupid] things will not become smart because they are in a list.
            \item[Smart] things, though, can be presented beautifully in a list.
        \end{description}
    \item Technical note: Viewing this in Chrome, however, will show too much vertical space
        at the end of a nested environment (see above). On top of that, margin collapsing for inline-block
        boxes is not allowed. Maybe using \texttt{dl} elements is too complicated for this and a simple nested
        \texttt{div} should be used instead.
\end{enumerate}
%
Lists can be deeply nested:
%
\begin{itemize}
  \item list text, level one
    \begin{itemize}
      \item list text, level two
        \begin{itemize}
          \item list text, level three

            And a new paragraph can be started, too.
            \begin{itemize}
              \item list text, level four

                And a new paragraph can be started, too.
                This is the maximum level.

              \item list text, level four
            \end{itemize}

          \item list text, level three
        \end{itemize}
      \item list text, level two
    \end{itemize}
  \item list text, level one
  \item list text, level one
\end{itemize}

\section{Mathematical Formulae}

Math is typeset using KaTeX. Inline math:
$
f(x) = \int_{-\infty}^\infty \hat f(\xi)\,e^{2 \pi i \xi x} \, d\xi
$
as well as display math is supported:
$$
f(n) = \begin{cases} \frac{n}{2}, & \text{if } n\text{ is even} \\ 3n+1, & \text{if } n\text{ is odd} \end{cases}
$$

</latex-js>

Full $$\LaTeX$$ blocks are supported through [LaTeX JS](https://latex.js.org/){:target="\_blank"} library, which is still under development and supports only limited functionality (which is still pretty cool!) and does not allow fine-grained control of the layout, fonts, etc.

*Note: We do not recommend using using LaTeX JS for writing lecture notes at this stage.*

***

## Print

Finally, you can easily get a PDF or printed version of the notes by simply hitting `ctrl+P` (or `⌘+P` on macOS).

