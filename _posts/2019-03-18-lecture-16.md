---
layout: distill
title: "Lecture 16: Building Blocks of Deep Learning"
description: Overview of CNNs, RNNs, and attention.
date: 2019-03-18

lecturers:
  - name: Zhiting Hu

authors:
  - name: Scott Sun
  - name: Raunaq Bhirangi
  - name: Author 3
    url: "#"

editors:
  - name: Editor 1  # editor's full name
    url: "#"  # optional URL to the editor's homepage

---

# Convolutional Neural Networks (CNNs)

CNNs are biologically-inspired variants of MLPs that exploit the strong spatial local correlations present in images.
The biological concept of the receptive field states that the visual cortex contains a complex arrangement of cells
that are sensitive to small sub-regions that are tiled to cover the visual field.
CNNs enjoy sparse connectivity, shared weights, and a hierarchy of representation. Stacking multiple layers can result in
lower layers learning low-level features, while upper layers learn high-level representations. Using the biological analogue,
simple cells detect local features while complex ones pool the outputs of simpler cells.

<figure id="example-figure" class="l-body-outset">
  <div class="row">
    <div class="col two">
      <img src="{{ 'assets/img/notes/lecture-16/cnn_hierarchy.png' | relative_url }}" />
      <figcaption>
        <strong>Figure 1</strong>
        Hierarchy of Features in CNNs
      </figcaption>
    </div>
   </div>
</figure>

Examples of ConvNets show a trend towards ever increasing numbers of layers:
* AlexNet, 8 layers
* VGG, 19 layers
* GoogLeNet, 22 layers
* ResNet, 152 layers


# Recurrent Neural Networks (RNNs)
The temporal (or sequential) analogue to the CNN is the RNN. RNNs can have a variable number of computation steps unlike CNNs.
Unlike MLPs and CNNs, RNN outputs depend not only on the current input, but also on the previous states of hidden layers.


## LSTMs and the Vanishing/Exploding Gradient Problem
Unrolling an RNN for several steps results in multiple products with W and applying tanh multiple times. The hidden states
that are passed on to each successive cell follow this expression:

$$ h_t = tanh(W^{hh} h_{t-1} + W^{hx} x_t) $$

As you backpropagate backward to $h_0$, there will be many repeated factors of W and tanh. 
If the singular value of the W matrix is greater than 1, this can result in exploding gradients; similarly, singular values under 1 can result in vanishing gradients.
This is because product over the W matrices during backprop will result in either exponential growth/decay in value.
A solution to this problem is to use **gradient clipping**. In the case of exploding gradients, this involves checking if the 
norm of the gradient is greater than some threshold. If so, the gradient is scaled by the threshold divided by the norm.

<figure id="example-figure" class="l-body-outset">
  <div class="row">
    <div class="col two">
      <img src="{{ 'assets/img/notes/lecture-16/rnn_grad_flow.png' | relative_url }}" />
    </div>
   </div>
</figure>

LSTMs are designed to solve the long-term dependency problem by creating a path with uninterrupted gradient flow during backpropagation.
Internally, they are more complicated than a vanilla RNN.
<figure id="example-figure" class="l-body-outset">
  <div class="row">
    <div class="col two">
      <img src="{{ 'assets/img/notes/lecture-16/rnn_lstm.png' | relative_url }}" />
      <figcaption>
        <strong>Figure </strong>
        RNN vs LSTM
      </figcaption>
    </div>
   </div>
</figure>

They use linear memory cells and multiplicative gates to store read, write, and reset information.
* **Forget gate**: decides what must be removed from $h_{t-1}$. 

$$
f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f) 
$$

<figure id="example-figure" class="l-body-outset">
  <div class="row">
    <div class="col one">
      <img src="{{ 'assets/img/notes/lecture-16/forget_gate.png' | relative_url }}" />
    </div>
   </div>
</figure>

* **Input gate**: whether to write to cell and what information should be stored

<d-math block>
\begin{aligned}
    i_t &= \sigma(W_i \cdot [h_{t-1}, x_t] + b_i) \\
    \tilde{C}_t &= tanh(W_c \cdot [h_{t-1}, x_t] + b_c) 
\end{aligned}
</d-math>

<figure id="example-figure" class="l-body-outset">
  <div class="row">
    <div class="col one">
      <img src="{{ 'assets/img/notes/lecture-16/input_gate.png' | relative_url }}" />
    </div>
   </div>
</figure>

* **Update cell state**: $f_t$ controls how much of $C_{t-1}$ is forgotten. $i_t$ scales our new candidate values by how much we 
want to update each state value.

$$
C_t = f_t \times C_{t-1} + i_t \times \tilde{C}_t
$$

<figure id="example-figure" class="l-body-outset">
  <div class="row">
    <div class="col one">
      <img src="{{ 'assets/img/notes/lecture-16/lstm_update.png' | relative_url }}" />
    </div>
   </div>
</figure>

* **Output gate**: decides what to output from the cell state

<d-math block>
\begin{aligned}
    o_t &= \sigma(W_o \cdot [h_{t-1}, x_t] + b_o) \\
    h_t &= o_t \times tanh(C_t)
\end{aligned}
</d-math>
The sigmoid in $o_t$ decides which part of the cell state will be outputted. 

<figure id="example-figure" class="l-body-outset">
  <div class="row">
    <div class="col one">
      <img src="{{ 'assets/img/notes/lecture-16/output_gate.png' | relative_url }}" />
    </div>
   </div>
</figure>

As can be seen, LSTMs allow for a path with uninterrupted gradient flow, which helps mitigate the long-term dependency problem.
There is no need to multiply by the W matrix during backprop, which was the source of the growth/decay; 
instead, you multiply by the different values of the gates. 
While this does not totally eliminate the vanishing/exploding gradient problem, it makes it much less likely, 
as there is usually a path where the gradient does not explode/vanish.

<figure id="example-figure" class="l-body-outset">
  <div class="row">
    <div class="col two">
      <img src="{{ 'assets/img/notes/lecture-16/lstm_grad_flow.png' | relative_url }}" />
    </div>
   </div>
</figure>


## Different Flavors of RNNs

<figure id="example-figure" class="l-body-outset">
  <div class="row">
    <div class="col two">
      <img src="{{ 'assets/img/notes/lecture-16/rnn.png' | relative_url }}" />
      <figcaption>
        <strong>Figure </strong>
        Some Conventional Variants of RNNs
      </figcaption>
    </div>
   </div>
</figure>

* **Bi-directional RNN**: The hidden state is the concatenated result of both forward and backward hidden states so that 
it can capture past and future information.
<figure id="example-figure" class="l-body-outset">
  <div class="row">
    <div class="col one">
      <img src="{{ 'assets/img/notes/lecture-16/bilstm.png' | relative_url }}" />
    </div>
   </div>
</figure>

* **Tree-structured RNN**: Allows for the hidden states to be conditioned on an input and the hidden states of *arbitrarily*
many child units. The standard LSTM, for example, is a special case conditioned on only one child unit.
<figure id="example-figure" class="l-body-outset">
  <div class="row">
    <div class="col one">
      <img src="{{ 'assets/img/notes/lecture-16/tree_lstm.png' | relative_url }}" />
    </div>
   </div>
</figure>

* **RNNs for 2D Sequences**: Fixed structures for 2D images that capture different dependencies and hierarchical relations.
<figure id="example-figure" class="l-body-outset">
  <div class="row">
    <div class="col one">
      <img src="{{ 'assets/img/notes/lecture-16/2d_lstm.png' | relative_url }}" />
    </div>
   </div>
</figure>

* **RNNs for Graph Structures**: Used in image segmentation.
<figure id="example-figure" class="l-body-outset">
  <div class="row">
    <div class="col one">
      <img src="{{ 'assets/img/notes/lecture-16/graph_lstm.png' | relative_url }}" />
    </div>
   </div>
</figure>


# Attention Mechanisms

Attention mechanisms are techniques that are used to focus on particular features in the data. They has been show to drastically improve performance in tasks such as machine translation, image captioning and speech recognition. They allow to accomodate for long-range dependencies, and dealing with the problem of vanishing gradients, seen in RNNs. By allowing for fine-grained localized representations of portions of data, like patches in images or words in sentences, attention improves feature recognition in the model.

## Attention Computation 

Attention can be computed for a machine translation task using the following procedure: 
* Encode each token in the input sentence into a key vector.
* When decoding, compare the query vector with the encoder states, and generate alignment scores corresponding to each key vector.
* Compute the attention weights by normalizing the alignment scores.
* Treat the encoder states as value vectors and compute a weighted sum, using the attention weights.
* Use this weighted sum in the decoder to generate the next token.

## Attention Variants
There are a number of different alignment score functions that may be used to generate scores. Some of these are shown in the table below:

<figure id="att-variants" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="{{ 'assets/img/notes/lecture-16/att-variants.png' | relative_url }}" />
    </div>
   </div>
</figure>

Soft and hard attention are variants of attention that respectively use deterministic and stochastic methods in computing the weights for each token. The computation described above is for soft attention. Instead of using the attention weights to compute a weighted average, hard attention uses these as probabilities and samples from the corresponding features using this distribution. A comparison for attention used on images can be illustrated below. Notice how soft attention can be diffuse, and assign nonzero weights to significant weights to large portions of the image at times, while hard attention focuses on a particular equally-sized part of the image in each case. Soft attention is presently the more popular variant, primarily because it allows for simpler backpropagation in the network.

<figure id="soft-hard-att" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="{{ 'assets/img/notes/lecture-16/soft-hard.png' | relative_url }}" />
    </div>
   </div>
</figure>

## Applications in Computer Vision
Attention can be used in conjunction with conventional CNNs in image processing. Features extracted from the CNN are used as key vectors and attention is used to sequentialy compute the caption as tokens. It can also be used in image paragraph generation, which is the generation of a long paragraph to describe an image. This is a challenging task because it involves long-term reasoning about language and visual features. Each sentence needs to be grounded on visual features to ensure contentful descriptions. One technique for doing this was presented in the lecture and it proceeds as follow:
* The image is first segmented into semantic regions, and each region is locally captioned with phrases
* Attention is applied on the visual regions and the text phrases and the resulting features fed to a generator which generates sentences
* These sentences are then fed to a sentence discriminator and a recurrent topic-transition discriminator for assessing sentence plausibility and topic coherence respectively
* A paragraph description corpus is adopted to provide linguistic knowledge about paragraphgeneration, which depicts the true data distribution of the discriminators

The entire pipeline can be seen in the figure below:  
<figure id="img-paragraph" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="{{ 'assets/img/notes/lecture-16/img-paragraph.png' | relative_url }}" />
    </div>
   </div>
</figure>





# Transformers: Multi-Headed Attention



## Equations

This theme supports rendering beautiful math in inline and display modes using [KaTeX](https://khan.github.io/KaTeX/) engine.
You just need to surround your math expression with `$`, like `$ E = mc^2 $`.
If you leave it inside a paragraph, it will produce an inline expression, just like $E = mc^2$.

To use display mode, again surround your expression with `$$` and place it as a separate paragraph.
Here is an example:

$$
\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)
$$

Alternatively and for more complex math environments, use `<d-math block>...</d-math>` tags.
Here is an example:

<d-math block>
\begin{aligned}
\left( \sum_{i=1}^n u_i v_i \right)^2 & \leq \left( \sum_{i=1}^n u_i^2 \right) \left( \sum_{i=1}^n v_i^2 \right) \\
\left| \int f(x) \overline{g(x)} dx \right|^2 & \leq  \int |f(x)|^2 dx \int |g(x)|^2 dx
\end{aligned}
</d-math>

Note that [KaTeX](https://khan.github.io/KaTeX/) is work in progress, so it does not support the full range of math expressions as, say, [MathJax](https://www.mathjax.org/).
Yet, it is [blazing fast](http://www.intmath.com/cg5/katex-mathjax-comparison.php).

***

## Figures

To add figures, use `<figure>...</figure>` tags.
Within the tags, define multiple rows of images using `<div class="row">...</div>`.
To add captions, use `<figcaption>...</figcaption>` tags.

Here is an example usage of a figure that consists of a row of images with a caption:

<figure id="example-figure" class="l-body-outset">
  <div class="row">
    <div class="col one">
      <img src="{{ 'assets/img/notes/template/1.jpg' | relative_url }}" />
    </div>
    <div class="col one">
      <img src="{{ 'assets/img/notes/template/2.jpg' | relative_url }}" />
    </div>
    <div class="col one">
      <img src="{{ 'assets/img/notes/template/3.jpg' | relative_url }}" />
    </div>
  </div>
  <figcaption>
    <strong>Figure caption title in bold.</strong>
    An example figure caption.
  </figcaption>
</figure>

Note that the figure uses `class="l-body-outset"` which lets it take more horizontal space.
For more on this, see layouts section below.
Also, the size of the images themselves is controlled by `class="one"`, `class="two"`, or `class="three"` which corresponds to 1/3, 2/3, 3/3 of the full horizontal space, respectively.

Here is the same example, but each image is captioned separately:
<figure id="example-figure" class="l-body-outset">
  <div class="row">
    <div class="col one">
      <img src="{{ 'assets/img/notes/template/1.jpg' | relative_url }}" />
      <figcaption>
        <strong>Figure caption title 1.</strong>
        Caption text for figure 1.
      </figcaption>
    </div>
    <div class="col one">
      <img src="{{ 'assets/img/notes/template/2.jpg' | relative_url }}" />
      <figcaption>
        <strong>Figure caption title 2.</strong>
        A very very very long caption text for figure 2 so that it is longer than the image itself.
      </figcaption>
    </div>
  </div>
</figure>

Here is an example that shows how the figures of different sizes are aligned:

<figure>
  <div class="row">
    <div class="col two">
      <img src="{{ 'assets/img/notes/template/4.jpg' | relative_url }}" />
    </div>
    <div class="col one">
      <img src="{{ 'assets/img/notes/template/2.jpg' | relative_url }}" />
      <figcaption>
        <strong>Subcaption.</strong>
        The content of the subcaption.
      </figcaption>
    </div>
  </div>
  <figcaption>
    <strong>The second row figure caption title.</strong>
    An example of a sencond row figure caption.
  </figcaption>
</figure>

***

## Citations

Citations are then used in the article body with the `<d-cite>` tag.
The key attribute is a reference to the id provided in the bibliography.
The key attribute can take multiple ids, separated by commas.

The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover).
If you have an appendix, a bibliography is automatically created and populated in it.

Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover.
However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.

***

## Footnotes

Just wrap the text you would like to show up in a footnote in a `<d-footnote>` tag.
The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote>

***

## Code Blocks

Syntax highlighting is provided within `<d-code>` tags.
An example of inline code snippets: `<d-code language="html">let x = 10;</d-code>`.
For larger blocks of code, add a `block` attribute:

<d-code block language="javascript">
  var x = 25;
  function(x) {
    return x * x;
  }
</d-code>

***

## Layouts

The main text column is referred to as the body.
It is the assumed layout of any direct descendants of the `d-article` element.

<div class="fake-img l-body">
  <p>.l-body</p>
</div>

For images you want to display a little larger, try `.l-page`:

<div class="fake-img l-page">
  <p>.l-page</p>
</div>

All of these have an outset variant if you want to poke out from the body text a little bit.
For instance:

<div class="fake-img l-body-outset">
  <p>.l-body-outset</p>
</div>

<div class="fake-img l-page-outset">
  <p>.l-page-outset</p>
</div>

Occasionally you’ll want to use the full browser width.
For this, use `.l-screen`.
You can also inset the element a little from the edge of the browser by using the inset variant.

<div class="fake-img l-screen">
  <p>.l-screen</p>
</div>
<div class="fake-img l-screen-inset">
  <p>.l-screen-inset</p>
</div>

The final layout is for marginalia, asides, and footnotes.
It does not interrupt the normal flow of `.l-body` sized text except on mobile screen sizes.

<div class="fake-img l-gutter">
  <p>.l-gutter</p>
</div>

***

## Arbitrary $$\LaTeX$$ (experimental)

In fact, you can write entire blocks of LaTeX using `<latex-js>...</latex-js>` tags.
Below is an example:<d-footnote>If you don't see anything, it means that your browser does not support Shadow DOM.</d-footnote>

<latex-js style="border: 1px dashed #aaa;">
This document will show most of the supported features of \LaTeX.js.

\section{Characters}

It is possible to input any UTF-8 character either directly or by character code
using one of the following:

\begin{itemize}
    \item \texttt{\textbackslash symbol\{"00A9\}}: \symbol{"00A9}
    \item \verb|\char"A9|: \char"A9
    \item \verb|^^A9 or ^^^^00A9|: ^^A9 or ^^^^00A9
\end{itemize}

\bigskip

\noindent
Special characters, like those:
\begin{center}
\$ \& \% \# \_ \{ \} \~{} \^{} \textbackslash % \< \>  \"   % TODO cannot be typeset
\end{center}
%
have to be escaped.

More than 200 symbols are accessible through macros. For instance: 30\,\textcelsius{} is
86\,\textdegree{}F.
</latex-js>

Note that you can easily interleave latex blocks with the standard markdown.

<latex-js style="border: 1px dashed #aaa;">
\section{Environments}

\subsection{Lists: Itemize, Enumerate, and Description}

The \texttt{itemize} environment is suitable for simple lists, the \texttt{enumerate} environment for
enumerated lists, and the \texttt{description} environment for descriptions.

\begin{enumerate}
    \item You can nest the list environments to your taste:
        \begin{itemize}
            \item But it might start to look silly.
            \item[-] With a dash.
        \end{itemize}
    \item Therefore remember: \label{remember}
        \begin{description}
            \item[Stupid] things will not become smart because they are in a list.
            \item[Smart] things, though, can be presented beautifully in a list.
        \end{description}
    \item Technical note: Viewing this in Chrome, however, will show too much vertical space
        at the end of a nested environment (see above). On top of that, margin collapsing for inline-block
        boxes is not allowed. Maybe using \texttt{dl} elements is too complicated for this and a simple nested
        \texttt{div} should be used instead.
\end{enumerate}
%
Lists can be deeply nested:
%
\begin{itemize}
  \item list text, level one
    \begin{itemize}
      \item list text, level two
        \begin{itemize}
          \item list text, level three

            And a new paragraph can be started, too.
            \begin{itemize}
              \item list text, level four

                And a new paragraph can be started, too.
                This is the maximum level.

              \item list text, level four
            \end{itemize}

          \item list text, level three
        \end{itemize}
      \item list text, level two
    \end{itemize}
  \item list text, level one
  \item list text, level one
\end{itemize}

\section{Mathematical Formulae}

Math is typeset using KaTeX. Inline math:
$
f(x) = \int_{-\infty}^\infty \hat f(\xi)\,e^{2 \pi i \xi x} \, d\xi
$
as well as display math is supported:
$$
f(n) = \begin{cases} \frac{n}{2}, & \text{if } n\text{ is even} \\ 3n+1, & \text{if } n\text{ is odd} \end{cases}
$$

</latex-js>

Full $$\LaTeX$$ blocks are supported through [LaTeX JS](https://latex.js.org/){:target="\_blank"} library, which is still under development and supports only limited functionality (which is still pretty cool!) and does not allow fine-grained control of the layout, fonts, etc.

*Note: We do not recommend using using LaTeX JS for writing lecture notes at this stage.*

***

## Print

Finally, you can easily get a PDF or printed version of the notes by simply hitting `ctrl+P` (or `⌘+P` on macOS).

