---
layout: distill
title: "Lecture 02: Bayesian Networks"
description: TBD
date: 2019-01-16

lecturers:
  - name: Eric Xing
    url: "https://www.cs.cmu.edu/~epxing/"

authors:
  - name: Cathy Su
    url: "https://ceesu.github.io"   
  - name: Author 2
    url: "#"
  - name: Author 3
    url: "#"

editors:
  - name: Editor 1  # editor's full name
    url: "#"  # optional URL to the editor's homepage

abstract: >
  An example abstract block.
---

***

# Readings
## The Bayesian network representation <d-cite key="Koller:2009:PGM:1795555"></d-cite>

### 3.1.1  Exploiting independence Properties
#### Standard vs compact parametrization of  independent random variables
Given random variables $$X_i$$ each representing the outcome of an independent coin toss, the standard parametrization of their joint distribution would be as follows:

$$ P(X_1, ... X_n) = P(X_1 = x_1, X_2=x_2, ..., X_n = x_n) = P(X_1 = x_1)P(X_2=x_2)...P(X_n=x_n) $$

Note there are 2 possibilities for each outcome $$x_i$$, this representation requires $$2^n$$ total number of parameters. 

One simple way to reduce the number of parameters needed, would be to represent the probability that each coin toss lands heads as $$ \theta_1, ... \theta_n $$.  Then we have the following compact representation requiring only $$n$$ parameters:

$$ P(X_1, ... X_n) = \prod_i \theta_{X_i} $$

### 3.1.3 Naive Bayes
We can further express the joint distribution in terms of conditional probabilities. This is done in the Naive Bayes model.

 Instances of this model have will include:
* class : some category $$C \in \{c^1, ...c^k\}$$ which gives a prior on the value of each feature
* features: observed properties $$X_1, ... X_k$$ 

The model makes the strong  'naive' conditional independence assumption:

$$ P(x_i | C,  x_1, \dots, x_n) = P(x_i | C) $$

In words, _features are conditionally independent, given the class of the instance of this model_.  Thus the joint distribution of the Naive Bayes model factorizes as follows:

$$ P(C, X_1, ... X_n) = P(C) \prod_i P(X_i |C)$$

### 3.2.1 Bayesian networks
Unlike in the Naive Bayes model, Bayesian networks can also represent distributions that do not satisfy the naive conditional independence assumption. 

The Bayesian network is represented by a directed acyclic graph $$\mathcal{G}$$ which serves to represent the joint distribution compactly, by representing a set of conditional probability assumptions about the distribution.

### 3.2.3 Graphs and Distributions



### Box 3.C Knowledge engineering

 Building a Bayesian Network in the real world requires many steps including:

* _picking variables precisely_: we need some variables that can be observed, and their domain should be specified. Sometimes introducing hidden variables will help to render the observed variables conditionally independent.
* _picking a structure consistent with the causal order_: choose 
* _picking probability estimates for events in the network_: 

Done correctly, the model will be accurate as well as not too complex.
### 3.3.1 D-separation

### 3.3.2 Soundness and Completeness


Test image

<img src="{{ '/assets/img/notes/lecture-02/cmu-logo.png }}" />

***

# In class notes


