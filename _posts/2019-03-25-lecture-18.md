---
layout: distill
title: "Lecture 18: Deep Generative Models (Part 2)"
description: GANs and their variations, Normalizing Flows and Integrating domain knowledge in DL.
date: 2019-03-25

lecturers:
  - name: Zhiting Hu
    url: "https://www.cs.cmu.edu/~zhitingh/"

authors:
  - name: Ruohong Zhang
    url: "#"  # optional URL to the author's homepage
  - name: Elan Rosenfeld
    url: "#"
  - name: Yifan Sun
    url: "#"
  - name: Sebastian Caldas
    url: "#"

editors:
  - name: Lisa Lee
    url: "#"  # optional URL to the editor's homepage

---

<!-- Ruohong -->
## GANs and their variations

<!-- Elan -->
## Normalizing Flows

## Integrating domain knowledge in DL
<!-- Yifan -->
### Learning with Constraints

<!-- Sebastian -->
### Rule Knowledge Distillation

Knowledge distillation was first proposed as a way to compress (or distill) the knowledge of an ensemble of "teacher" neural networks into a single "student" network <d-cite key="hinton2015distilling"></d-cite>. This technique was motivated by two observations: (1) that ensemble methods tend to have better performance than their single-model counterpart, and (2) that deploying an ensemble of deep neural networks is not feasible at inference time (as every network needs to make a prediction).

In the context of integrating domain knowledge into DL, a variant of knowledge distillation has been proposed as a way to have the student network learn "soft constraints" from a teacher network <d-cite key="hu2016harnessing"></d-cite>. Unlike the original variant, for this purposes we only need a single teacher that is, nevertheless, trained in parallel to the student. 

<figure>
<img src="{{ '/assets/img/notes/lecture-18/knowledge_distillation.png' | relative_url }}" />
<figcaption>
Schematic view of the rule knowledge distillation technique proposed by <d-cite key="hu2016harnessing"></d-cite>.
</figcaption>
</figure>

For a multi-class prediction task, the Rule Knowledge Distillation technique works as follows:
1. A student network $p_{\theta}(y \mid x)$ parameterized by weights $\theta$ is trained to produce a prediction vector $\sigma_{\theta}(x)$.
2. At time step $t$, construct teacher network $q(y \mid x)$ by projecting $p_{\theta}(y \mid x)$ into a rule-regularized space. In other words, for a set of rules $(R_l, \lambda_l), l=1, ..., L$ our goal is to find $q(y \mid x)$ such that, for every rule $l$ (specified using the soft logic in <d-cite key="bach2015hinge"></d-cite>), all rule groundings $r_{l, g_l}(X, Y), g=1, ..., G_l$ are satisfied in expectation with confidence $\lambda_l$. More formally, we wish to solve the following optimization problem,
    <d-math block>
    \begin{aligned}
    \min_{q, \xi \geq 0} KL(q(Y \mid X) \mid \mid p_{\theta}(Y \mid X)) + C \sum_{l, g_l} \xi_{l, g_l} \\
    \text{s.t.} \, \lambda_l (1 - \mathbb{E}[r_{l, g_l}(X, Y)]) \leq \xi_{l, g_l} \\
    g_l = 1, ..., G_l, l = 1, ..., L.
    \end{aligned}
    </d-math>
where $\xi_{l, g_l}$ is a slack variable for each rule constraint and $C$ is a regularization parameter. Now, this problem has closed a form solution:
    <d-math block>
    \begin{aligned}
    q^{\star}(Y \mid X) \propto p_{\theta}(Y \mid X) \exp{\left(-\sum_{l, g_l} C \lambda_l (1 - r_{l, g_l}(X, Y))\right)}
    \end{aligned}
    </d-math>
3. Now, $p_{\theta}(y \mid x)$ is further trained to emulate the behavior of $q^{\star}$. This is achieved by reformulating the objective of $p_{\theta}(y \mid x)$ as
    <d-math block>
    \begin{aligned}
    \theta^{(t+1)} = \text{arg}\min_{theta} \frac{1}{N} \sum_{n=1}^N (1 - \pi) \ell(y_n, \sigma_\theta(x_n)) + \pi \ell(s_n, \sigma_\theta(x_n)) 
    \end{aligned}
    </d-math>
where $x_n, y_n, n=1, ..., N)$ are the labeled training samples, $\ell$ is the loss function at hand and $s_n$ is the prediction of $q^{\star}$ at $x_n$. $\pi$ is an imitation parameter that calibrates the importance of the student network learning the hard labels $y_n$ vs. it imitating the behavior of $q^{\star}$. In <d-cite key="hu2016harnessing"></d-cite>, $\pi$ was set to $\pi^{(t)} = \min(\pi_0, 1-\alpha^t)$ for some lower bound $\pi_0 < 1$ and speed of decay $\alpha \leq 1$. This means that, at the beginning of training, $p$ is encouraged to learn the hard labels, behavior that eventually fades in favor of imitating $q^{\star}$.  
1. Repeat the last two steps (alternating between training $q$ and refining $p$) until convergence.


As a final observation, notice that the second and third steps could be interpreted as the $E$ and $M$ steps in an $EM$ algorithm.

### Incomplete or noisy domain knowledge

So far, domain knowledge has been specified through either a set of rules $(R_l, \lambda_l), l=1, ..., L$, which require confidence parameters $\lambda_l$, or through a constraint function $f_{\phi}(x)$, which has to be constructed a priori (i.e., before training). Recent work, however, allows for more flexible specifications:

* The work on <d-cite key="hu2016deep"></d-cite> learns the confidence parameters $\lambda_l$ for a given set of rules. This means that the knowledge encoded through these rules can be noisy, as the algorithm will learn to ignore false rules.
* The work on <d-cite key="hu2018deep"></d-cite> allows for the constraint function $f_{\phi}(x)$ to have learnable components. These components are interpreted as the reward function in a reinforcement learning setting, meaning they can be learned using techniques from inverse reinforcement learning. 
