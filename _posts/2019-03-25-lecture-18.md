---
layout: distill
title: "Lecture 18: Deep Generative Models (Part 2)"
description: GANs and their variations, Normalizing Flows and Integrating domain knowledge in DL.
date: 2019-03-25

lecturers:
  - name: Zhiting Hu
    url: "https://www.cs.cmu.edu/~zhitingh/"

authors:
  - name: Ruohong Zhang
    url: "#"  # optional URL to the author's homepage
  - name: Elan Rosenfeld
    url: "#"
  - name: Yifan Sun
    url: "#"
  - name: Sebastian Caldas
    url: "#"

editors:
  - name: Lisa Lee
    url: "#"  # optional URL to the editor's homepage

---

<!-- Ruohong -->
## GANs and their variations
### GAN Progress
From 2014 to 2018, GAN can generate more high quality and more realistic images.
<figure>
<img src="{{ '/assets/img/notes/lecture-18/gan_progress.jpeg' | relative_url }}" />
<figcaption>
The changing of quality of image generation using GAN. 
Figure courtesy: Ian Goodfellow
</figcaption>
</figure>

### Vanilla GAN Review
In GAN setting, we have a generative model and a discriminator model. The generative model trys to generate new data given a prior, 
while the discriminative model distinguishes the difference between the generated data and the true data.


The **generative** part of GAN model maps a random noise to a generated data, i.e. $\bm{x} = G_\theta(\bm{z}), \bm{z} \sim p(\bm{z})$, where $z$ is a random noise and $x$ is the data space.
It defines a implicit distribution over $x: p_{g_\theta}(x)$, and uses a stochastic process to simulate data $x$. It is intractable to evaluate the likelihood of generated data.

The **descriminative** part of GAN tells apart the true data distibution from the generated distribution. Specifically, the descrimination $D_\phi (x)$ outputs a probability of $x$ being the true data.
 
The **learning objective** of GAN can be formulated as a minimax game between the generator and the discriminator. It simultaneously trains a discriminator $D$ to maximize the probability of assigning the correct label to both training examples and generated samples, and trains a generator $G$ to fool the discriminator.
<d-math block>
\min_{G} \max _{D} V(D, G)=E_{\boldsymbol{x} \sim p_{\text { data }}(\boldsymbol{x})}[\log D(\boldsymbol{x})]+ E_{\boldsymbol{z} \sim p_{\boldsymbol{z}}(\boldsymbol{z})}[\log (1-D(G(\boldsymbol{z})))]
</d-math>
Example:
<figure>
<img src="{{'/assets/img/notes/lecture-18/gan_pipeline.png' | relative_url }}" />
<figcaption>
GAN pipeline on MNIST Data
</figcaption>
</figure>

### Wasserstein GAN(WGAN)
In the setting of vanilla GAN, if the data and model's manifold are different, there can be negligible intersection between the two in practice. In this case, the loss function and gradients may not be continous or well-behaved. For example, for some $x$ such that $P_g(x)=0$ but $P_d(x) > 0$, KL divergence is infinite.

In order to solve the problem, **WGAN** <d-cite key="wgan"></d-cite> was proposed, in which a new objective was used by changing the distance function to Wasserstein Distance(Earth Mover). It measures the minimum transportation cost for making one pile of dirt in the shape of one probability distribution to the shape of the other distribution.

The hard part of learning objective is to ensure that $\|D\|_{L} \leq K$ K-Lipschitz continuous. In the paper, it uses gradient-clipping to ensure $D$ has the Lipschitz continuity, which was acknowledged by the author that it may be a terrible way to do so, but it is simple, yet still effective, and performs better the vanilla GAN.
<d-math block>
W (p_d, p_g) = {1 \over K} \cdot sup_{\|D\|_{L} \leq K} E_{x \sim p_d}[D(x)] - E_{x \sim p_g}[D(x)]
</d-math>


During the training, we compare the GAN discriminator with the WGAN critic when learning to differentiate two Gaussians. As we can see, the traditional GAN
discriminator saturates and results in vanishing gradients, whereas WGAN critic provides very clean gradients on all parts of the space.

<figure>
<img src="{{'/assets/img/notes/lecture-18/wgan_training.png' | relative_url }}" />
<figcaption>
WGAN Training
</figcaption>
</figure>

### Progressive GAN
Although GAN can produce sharp images, in most of the cases those images are in small resolutions with limited variation. Directly generating large image is impractable because the training becomes unstable and the model is hard to train. Due to those problems, the **Progressive GAN** <d-cite key="pgan"></d-cite> was proposed to improve the training method for large images. 

During the training phase, low resolution images were generated first, and then it progressively increases the resolution by adding new layers. The benefit of this training process is that the model could first detect large structure of image, and then shift its attention to finer details. The results are better generated images.

<figure>
<img src="{{'/assets/img/notes/lecture-18/pgan.png' | relative_url }}" />
<figcaption>
Progressive GAN Illustration
</figcaption>
</figure>

### BigGAN
Similar to Progressive GAN, the **BigGAN**<d-cite key="biggan"></d-cite> also targets at generating high fidelity, diverse images with models directly learned from data. The authors found that during the training process, GANs benefits dramatically from scaling. For example, in the paper, the model uses 2xâ€“4x more parameters, and 8x larger batch size. To avoid the model collapse, a strong discriminator is used first, and then it is relaxed towards the end of training.

<figure>
<img src="{{'/assets/img/notes/lecture-18/biggan.png' | relative_url }}" />
<figcaption>
BigGAN generating high resolution image
</figcaption>
</figure>

<!-- Elan -->
## Normalizing Flows
### Overview
Normalizing Flows transform a simple, usually factored distribution into a complex distribution through a sequence of smooth, invertible functions.

<figure>
<img src="{{ '/assets/img/notes/lecture-18/normalizing-flow.png' | relative_url }}" />
<figcaption>
Transformation from a simple base distribution to a complex approximation to the true distribution. Figure courtesy: Lilian Weng
</figcaption>
</figure>

### Example Density Transformations
Individual functions are restricted in architecture to ensure the tractability of the model. Such restrictions mean that each function alone is often quite simple, but a long sequence can result in arbitrarily complex target distributions.

<figure>
<img src="{{ '/assets/img/notes/lecture-18/flow-examples.png' | relative_url }}" />
<figcaption>
Two different classes of transformations applied to 2-dimensional isotropic Gaussian and Uniform base distributions. Note the growth in complexity as the number of sequential function applications grows. [Rezende & Mohamed, 2015]
</figcaption>
</figure>

### Benefits of Normalizing Flows:
**Inference is Easy**
* Apply inverse transformation to input and evaluate the probability of the result with respect to the base distribution.
* Makes use of the inverse function theorem: If $$x = f(z)$$, then
$$p(x) = p(z) \left|\det\frac{\partial f^{-1}}{\partial x}\right| = p(z)\left|\det\frac{\partial f}{\partial z}\right|^{-1}$$
* Requires the transformation $$f$$ to be invertible and have a Jacobian with a tractable determinant.
* Architecture of $$f$$ is typically chosen to be autoregressive, which means the Jacobian will be triangular.

**Sampling is Easy**
* Sample from the base distribution and push through the transformation.

### Trained by Maximizing Log-Likelihood
$$\log p(x) = \log p(z_0) + \sum_{i=1}^K \log\left|\det\frac{dz_{i-1}}{dz_i}\right|$$

### Current State of the Art: GLOW
* Builds off ideas of NICE and Real NVP, two of the first papers to use normalizing flows for deep generative modeling.
* Generalizes the permutation operation of Real NVP with a 1x1 convolution of "channels".
* Produces high-quality samples.
* Not yet on par with GANs, but we get exact log-likelihood evaluation!

<figure>
<img src="{{ '/assets/img/notes/lecture-18/glow.png' | relative_url }}" />
<figcaption>
Samples from the GLOW model. [Kingma and Dhariwal, 2018]
</figcaption>
</figure>

## Integrating domain knowledge in DL
<!-- Yifan -->
### Integrating Domain Knowledge into Deep Learning:
* Deep learning heavily relies on massive labeled data.
* DL is hard to interpret and does not encode domain knowledge.
* Want to integrate our domain knowledge into the DL algorithm.

### Set Up of Constraint Learning
* Consider a generative model $x\sim p_\theta(x)$ with objective function $\mathcal{L}(\theta)$. 
* Domain knowledge is represented by a constraint function $f_\theta(x)\in R$.
* Higher $f$ values means better knowledge on $x$. 

### Direct Objective Optimization and Difficulties
* The way to impose constraint is to maximize $E_{p_\theta}[f(x)]$.
* This term could be added as a regularization term, which leads to the optimization problem:
$\min_\theta \mathcal{L}(\theta) - \alpha E_{p_\theta}[f(x)]$. 
* In general,$E_{p_\theta}[f]$ is hard to optimize unless $p_\theta$ is a GAN-like implicit generative model or an explicit distribution that can be efficiently reparametrized.
For other models such as the large set of non-reparametrizable explicit distributions, the gradient $\nabla E_{p_\theta}[f_\theta(x)]$ is usually computed with the log-derivative trick and can suffer from high variance. 

### Variational Method
* Instead of optimizing $E_{p_\theta}[f_\theta(x)]$, we can introduce a variational distribution $q$ which is encouraged to stay close to $p_\theta(x)$, we consider the following term 
$\mathcal{L}(\theta,q)=KL(q(x)\|p_\theta(x))-\alpha E_q[f]$ as proxy of $- E_{p_\theta}[f]$.
* The objective now becomes 
$$\min_\theta \mathcal{L}(\theta) + \alpha\mathcal{L}(\theta,q)$$.
* The problem could be solve by EM algorithm. 
* In E step, one optimize $\mathcal{L}(\theta,q)$ with respect to $q$ and get $$q*(x)=p_\theta(x)\exp{\alpha f(x)} / Z$$ where $Z$ is the normalization term.
* In M step, one plugs in $q^*$ and optimize w.r.t. $\theta$ with:
$$\min_\theta KL(q(x)\|p_\theta(x))=\min_\theta -E_q[\log p_\theta(x)] + const$$


## Learning with Constraints
### Learning the Knowledge Constraint
* In previous formulation, constraint $f(x)$ has to be fully-specified a priori and is fixed throughout the learning. It would be desirable or necessary to enable learnable constraint.
* We would like to let constraint function has learnable components, we write the constraint as $f_\phi(x)$ where $\phi$ are learnable parameters. 
* Learning $\phi$ is not straightforward, if we optimize $\phi$ in M-step, i.e. 
  $$\max_\phi E_{x\sim q(x)}[f_\phi(x)]$$, then such objective could be problematic becuase poor state of the generative parameter $\theta$ at the initial stage.
* One can treat learning of constraint as an extrinsic reward motived by Reinforcement Learning, and get a joint learning algorithm for $\theta$ and $\phi$. For details, please refer to Hu et.al. <d-cite key="hu2018deep"></d-cite>.


<!-- Sebastian -->
### Rule Knowledge Distillation

Knowledge distillation was first proposed as a way to compress (or distill) the knowledge of an ensemble of "teacher" neural networks into a single "student" network <d-cite key="hinton2015distilling"></d-cite>. This technique was motivated by two observations: (1) that ensemble methods tend to have better performance than their single-model counterpart, and (2) that deploying an ensemble of deep neural networks is not feasible at inference time (as every network needs to make a prediction).

In the context of integrating domain knowledge into DL, a variant of knowledge distillation has been proposed as a way to have the student network learn "soft constraints" from a teacher network <d-cite key="hu2016harnessing"></d-cite>. Unlike the original variant, for this purposes we only need a single teacher that is, nevertheless, trained in parallel to the student. 

<figure>
<img src="{{ '/assets/img/notes/lecture-18/knowledge_distillation.png' | relative_url }}" />
<figcaption>
Schematic view of the rule knowledge distillation technique proposed by <d-cite key="hu2016harnessing"></d-cite>.
</figcaption>
</figure>

For a multi-class prediction task, the Rule Knowledge Distillation technique works as follows:
1. A student network $p_{\theta}(y \mid x)$ parameterized by weights $\theta$ is trained to produce a prediction vector $\sigma_{\theta}(x)$.
2. At time step $t$, construct teacher network $q(y \mid x)$ by projecting $p_{\theta}(y \mid x)$ into a rule-regularized space. In other words, for a set of rules $(R_l, \lambda_l), l=1, ..., L$ our goal is to find $q(y \mid x)$ such that, for every rule $l$ (specified using the soft logic in <d-cite key="bach2015hinge"></d-cite>), all rule groundings $r_{l, g_l}(X, Y), g=1, ..., G_l$ are satisfied in expectation with confidence $\lambda_l$. More formally, we wish to solve the following optimization problem,
    <d-math block>
    \begin{aligned}
    \min_{q, \xi \geq 0} KL(q(Y \mid X) \mid \mid p_{\theta}(Y \mid X)) + C \sum_{l, g_l} \xi_{l, g_l} \\
    \text{s.t.} \, \lambda_l (1 - E[r_{l, g_l}(X, Y)]) \leq \xi_{l, g_l} \\
    g_l = 1, ..., G_l, l = 1, ..., L.
    \end{aligned}
    </d-math>
where $\xi_{l, g_l}$ is a slack variable for each rule constraint and $C$ is a regularization parameter. Now, this problem has closed a form solution:
    <d-math block>
    \begin{aligned}
    q^{\star}(Y \mid X) \propto p_{\theta}(Y \mid X) \exp{\left(-\sum_{l, g_l} C \lambda_l (1 - r_{l, g_l}(X, Y))\right)}
    \end{aligned}
    </d-math>
3. Now, $p_{\theta}(y \mid x)$ is further trained to emulate the behavior of $q^{\star}$. This is achieved by reformulating the objective of $p_{\theta}(y \mid x)$ as
    <d-math block>
    \begin{aligned}
    \theta^{(t+1)} = \text{arg}\min_{theta} \frac{1}{N} \sum_{n=1}^N (1 - \pi) \ell(y_n, \sigma_\theta(x_n)) + \pi \ell(s_n, \sigma_\theta(x_n)) 
    \end{aligned}
    </d-math>
where $x_n, y_n, n=1, ..., N)$ are the labeled training samples, $\ell$ is the loss function at hand and $s_n$ is the prediction of $q^{\star}$ at $x_n$. $\pi$ is an imitation parameter that calibrates the importance of the student network learning the hard labels $y_n$ vs. it imitating the behavior of $q^{\star}$. In <d-cite key="hu2016harnessing"></d-cite>, $\pi$ was set to $\pi^{(t)} = \min(\pi_0, 1-\alpha^t)$ for some lower bound $\pi_0 < 1$ and speed of decay $\alpha \leq 1$. This means that, at the beginning of training, $p$ is encouraged to learn the hard labels, behavior that eventually fades in favor of imitating $q^{\star}$.  
1. Repeat the last two steps (alternating between training $q$ and refining $p$) until convergence.


As a final observation, notice that the second and third steps could be interpreted as the $E$ and $M$ steps in an $EM$ algorithm.

### Incomplete or noisy domain knowledge

So far, domain knowledge has been specified through either a set of rules $(R_l, \lambda_l), l=1, ..., L$, which require confidence parameters $\lambda_l$, or through a constraint function $f_{\phi}(x)$, which has to be constructed a priori (i.e., before training). Recent work, however, allows for more flexible specifications:

* The work on <d-cite key="hu2016deep"></d-cite> learns the confidence parameters $\lambda_l$ for a given set of rules. This means that the knowledge encoded through these rules can be noisy, as the algorithm will learn to ignore false rules.
* The work on <d-cite key="hu2018deep"></d-cite> allows for the constraint function $f_{\phi}(x)$ to have learnable components. These components are interpreted as the reward function in a reinforcement learning setting, meaning they can be learned using techniques from inverse reinforcement learning. 
